{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update and import packages\n",
    "!pip install -Uqr requirements.txt\n",
    "\n",
    "import importlib\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from progress.bar import Bar\n",
    "import json\n",
    "import re\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "nltk.download(\n",
    "    [\n",
    "        'punkt',\n",
    "        'wordnet',\n",
    "        'tagsets',\n",
    "        'averaged_perceptron_tagger'\n",
    "    ], quiet=True\n",
    ")\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import vector_similarity\n",
    "importlib.reload(vector_similarity)\n",
    "from vector_similarity import VectorSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['a', 'b', 'c']], dtype='<U1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on VectorSimilarity\n",
    "# check_estimator(VectorSimilarity())\n",
    "X = np.array(\n",
    "    [[0, 1],\n",
    "     [1, 0],\n",
    "     [-1, 0]])\n",
    "y = np.array(['a', 'b', 'c'])\n",
    "\n",
    "estimator = VectorSimilarity()\n",
    "estimator = estimator.fit(X, y)\n",
    "estimator.predict(np.array([1, 2]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'b' 'c' 'd']\n",
      " ['b' 'a' 'c' 'd']\n",
      " ['c' 'b' 'a' 'd']\n",
      " ['d' 'c' 'b' 'a']]\n",
      "[[1.         0.50443175 0.3494023  0.        ]\n",
      " [1.         0.50443175 0.3494023  0.        ]\n",
      " [1.         0.3494023  0.3494023  0.        ]\n",
      " [1.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Basic pipeline setup\n",
    "basic_corpus = [\n",
    "    'Bees like to make honey',\n",
    "    'Bears like to eat honey',\n",
    "    'Bees don\\'t like bears',\n",
    "    'Humans are walking around the park'\n",
    "]\n",
    "basic_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    VectorSimilarity()\n",
    ")\n",
    "pipe.fit(basic_corpus, basic_labels)\n",
    "print(pipe.predict(basic_corpus))\n",
    "print(pipe.score(basic_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://github.com/aws-amplify/amplify-adminui/issues/12'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/21'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/67'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/82'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/41'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/28'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/85'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/45'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/35'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/33']]\n",
      "[[1.         0.6949403  0.23125501 0.13250384 0.12011294 0.12004747\n",
      "  0.11126224 0.08492276 0.08372554 0.07978957]]\n",
      "['https://github.com/aws-amplify/amplify-adminui/issues/12']\n"
     ]
    }
   ],
   "source": [
    "# Train and infer on small Data Wrangler dataset\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "chunksize = 1000\n",
    "output_content_type = \"parquet\"\n",
    "flow_export_id = f\"30-23-06-49-58efbaf1\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\"\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "elif output_content_type.upper() == \"PARQUET\":\n",
    "    dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "wrangled_df = next(dfs)\n",
    "\n",
    "X = wrangled_df['bodyText']\n",
    "y = wrangled_df['url']\n",
    "pipe.fit(X, y)\n",
    "\n",
    "print(pipe.predict(X[13:14]))\n",
    "print(pipe.score(X[13:14]))\n",
    "print(list(y[13:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data parquets\n",
    "secret_name = \"SageMakerS3Access\"\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "secrets = boto3.client(\n",
    "    service_name='secretsmanager',\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "(access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "bucket_name = 'githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v'\n",
    "bucket_subfolder = 'data/'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "data_obj_names = [key['Key'] for key in data_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File helper functions\n",
    "def list_data_objs():\n",
    "    secret_name = \"SageMakerS3Access\"\n",
    "    region_name = \"us-west-2\"\n",
    "    bucket_name = 'githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v'\n",
    "    bucket_subfolder = 'data/'\n",
    "    \n",
    "    secrets = boto3.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "    secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "    (access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "    data_obj_names = [f\"s3://{bucket_name}/{key['Key']}\" for key in data_objects]\n",
    "    \n",
    "    return data_obj_names\n",
    "\n",
    "def combine_dfs(dfs):\n",
    "    df = pd.concat(\n",
    "            dfs,\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Clear empty values and reset indices\n",
    "    df = df[(not isinstance(df.bodyText, str)) and (df.bodyText != '')]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def download_data(filename, data_obj_names):\n",
    "    dfs = []\n",
    "    \n",
    "    with Bar(\n",
    "        message='Downloading parquets',\n",
    "        check_tty=False,\n",
    "        hide_cursor=False,\n",
    "        max=len(data_obj_names)\n",
    "    ) as bar:\n",
    "\n",
    "        for obj_name in data_obj_names:\n",
    "            df = wr.s3.read_parquet(obj_name)\n",
    "            dfs.append(df)\n",
    "            bar.next()\n",
    "\n",
    "        bar.finish()\n",
    "        \n",
    "    df = combine_dfs(dfs)\n",
    "    return df\n",
    "\n",
    "def get_data(filename, force_redownload=False):\n",
    "    start = time()\n",
    "    data = Path(filename)\n",
    "    \n",
    "    if data.is_file() and not force_redownload:\n",
    "        print('Deserializing data from', filename, '...')\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "    else:\n",
    "        data_obj_names = list_data_objs()\n",
    "        df = download_data(filename, data_obj_names)\n",
    "        \n",
    "    print('Took', time() - start, 'seconds')    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data helper functions\n",
    "def query_df(df, **kwargs):\n",
    "    query = True\n",
    "    for key, value in kwargs.items():\n",
    "        query &= (df[key] == value)\n",
    "        \n",
    "    result = df[query]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cognito', 'NN')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model helper functions\n",
    "def infer(pipe, text, show_score=False):\n",
    "    print('Inferring on the query:', text)\n",
    "    start = time()\n",
    "    if type(text) == str:\n",
    "        text = list(text)\n",
    "        \n",
    "    print(pipe.predict(text))\n",
    "    \n",
    "    if show_score:\n",
    "        print(pipe.score(text))\n",
    "    print('Took', time() - start, 'seconds')\n",
    "    \n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = re.findall(r\"(?u)\\b\\w\\w+\\b\", doc)\n",
    "        tokens = [word for word in tokens if len(word) >=3]\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        tokens = [word for word, tag in tags if tag[0] in ['V', 'N']]\n",
    "        \n",
    "        return [self.wnl.lemmatize(t) for t in tokens]\n",
    "#         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "def trained_model(df):\n",
    "    corpus = df[corpus_col]\n",
    "    labels = df[label_col]\n",
    "\n",
    "    pipe = make_pipeline(\n",
    "        TfidfVectorizer(tokenizer=LemmaTokenizer()),\n",
    "    #     TfidfVectorizer(),\n",
    "        VectorSimilarity()\n",
    "    )\n",
    "    pipe.fit(corpus, labels)\n",
    "    return pipe\n",
    "\n",
    "# nltk.pos_tag(['Cognito'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading parquets |################################| 426/426Downloading parquets |##############                  | 193/426Downloading parquets |############################    | 385/426\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 77.18016958236694 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download and compile parquets\n",
    "df = get_data('training_data.csv', force_redownload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training data\n",
    "corpus_col = 'bodyCleaned'\n",
    "label_col = 'url'\n",
    "\n",
    "begin_text = r'.*Describe the bug'\n",
    "mid_text = r'### Expected behavior|### Reproduction steps|\\r\\n*'\n",
    "end_text = r'### Code Snippet.*'\n",
    "begin_text_feat = r'.*Describe the feature you\\'d like to request'\n",
    "mid_text_feat = r'### Describe the solution you\\'d like|### Describe alternatives you\\'ve considered'\n",
    "end_text_feat = r'### Additional context.*'\n",
    "\n",
    "cases = [\n",
    "    begin_text,\n",
    "    mid_text,\n",
    "    end_text,\n",
    "    begin_text_feat,\n",
    "    mid_text_feat,\n",
    "    end_text_feat\n",
    "]\n",
    "pat_cases = '(' + '|'.join(cases) + ')'\n",
    "pat = re.compile(pat_cases, flags=(re.DOTALL | re.M))\n",
    "\n",
    "js_df = query_df(df, repository='amplify-js')\n",
    "clean_template = lambda text : re.sub(pat, '', text)\n",
    "js_df[corpus_col] = js_df['title'] + ' ' + js_df['body'].apply(clean_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 40.676000118255615 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model (All default)\n",
    "start = time()\n",
    "\n",
    "small_df = js_df\n",
    "corpus = small_df[corpus_col]\n",
    "labels = small_df[label_col]\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        tokenizer=LemmaTokenizer(),\n",
    "#         stop_words='english'\n",
    "    ),\n",
    "    VectorSimilarity()\n",
    ")\n",
    "pipe.fit(corpus, labels)\n",
    "\n",
    "print('Training took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab words: 22661\n",
      "Available repos: ['aws-sdk-ios', 'docs', 'amplify-js', 'amplify-android', 'amplify-codegen', 'amplify-console', 'amplify-cli', 'amplify-ci-support', 'amplify-js-samples', 'amplify-flutter', 'amplify-ios', 'amplify-adminui', 'aws-sdk-android']\n"
     ]
    }
   ],
   "source": [
    "# Model stats\n",
    "vocab = pipe['tfidfvectorizer'].get_feature_names()\n",
    "print('Number of vocab words:', len(vocab))\n",
    "repo_list = list(set(df['repository']))\n",
    "print('Available repos:', repo_list)\n",
    "print()\n",
    "# print([word for word in vocab if len(word) < 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('extra_tokens.txt', 'w') as f:\n",
    "#     f.write(str(set(vocab2) - set(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['Password managers']\n",
      "[['https://github.com/aws-amplify/amplify-js/issues/8289'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/8472'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7919'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3522'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/14'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7957'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/5782'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/5915'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3661'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2895']]\n",
      "[[0.75959288 0.48289474 0.31851816 0.28561063 0.28403942 0.25993695\n",
      "  0.25335673 0.24394455 0.2402166  0.23787023]]\n",
      "Took 0.9714035987854004 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['Password managers']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['image file upload fail file size 5 mb']\n",
      "[['https://github.com/aws-amplify/amplify-js/issues/2977'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/6419'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2619'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7117'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/125'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/788'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7095'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3243'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7574'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/8325']]\n",
      "[[0.33228035 0.32719758 0.31901044 0.31800867 0.31663403 0.30111809\n",
      "  0.28300161 0.27654007 0.27037782 0.26808085]]\n",
      "Took 0.9504232406616211 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['image file upload fail file size 5 mb']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['user endpoint disappear pinpoint']\n",
      "[['https://github.com/aws-amplify/amplify-js/issues/6896'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3819'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4573'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7675'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/1777'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4529'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3126'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4712'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3886'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2544']]\n",
      "[[0.31512797 0.30891221 0.30699227 0.30641615 0.30504074 0.28408942\n",
      "  0.27650906 0.27225218 0.27203202 0.24243379]]\n",
      "Took 0.9509885311126709 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['user endpoint disappear pinpoint']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect to External Cognito Account **Is your feature request related to a problem? Please describe.**We are building multiple apps in a multi-account enterprise AWS environment and we're trying to consume a central cognito pool that is under a different account other than the amplify apps (each amplify app is under it's own account following best practices). Hence when we try to import the cognito pool for auth the IAM user cannot see that in the Role that is used by amplify. We looked at and successfully established cross account access using the method described [here](https://docs.amplify.aws/cli/usage/iam) however we cannot change roles after we ran amplify init so the end result is that we are provisioning the amplify app under the aws account that holds the central cognito instance. This issue is also mentioned [here](https://github.com/aws-amplify/amplify-cli/issues/7008).**Describe the solution you'd like**A straightforward way or recommended best practices for such scenario. We are building 2 separate apps and have a multiple others lined up in the queue. It is very important for us to get this straight before we commit more to amplify as a framework for all our apps.**Describe alternatives you've considered**Few looked at a few workarounds:1) We created a separate cognito pool and imported the the main cognito account through OIDC. - In this case login works but the user records are \"duplicated\" in the pools created by the amplify apps. - The access token that is issued for the apps does not belong to our central identity because it is issued by the amplify pool.- We are limited to use federated login in our mobile apps.2) We set the `@auth` directive on our AppSync schema to `oidc`, obtaining the token directly from the main pool through federated login.- In this case we cannot use Datastore (or just couldn't get it to work)- Other resources that don't support OIDC access will be difficult to work with.- In general seems like amplify expects a user pool imported, otherwise it looses most of it's convenience features.3) Abandon amplify completely, configure AppSync with `oidc` and other resources \"manually\".- If we decide to use `oidc` for AppSync auth (as far as we see now) there isn't much that amplify brings to the table. We can just setup our backend \"manually\" and keep the apps lightweight.- Obviously this is a lot more work than the other options and we would not pursue it if possible**Additional context**We would really like to use amplify and we leverage all it has, this it is really frustrating that that we cannot bypass that one step of importing the cognito pool. If this feature is something that will not be supported, we would still be happy to receive some feedback/direction on our workarounds above.\n",
      "Connect to External Cognito Account **Is your feature request related to a problem? Please describe.**We are building multiple apps in a multi-account enterprise AWS environment and we're trying to consume a central cognito pool that is under a different account other than the amplify apps (each amplify app is under it's own account following best practices). Hence when we try to import the cognito pool for auth the IAM user cannot see that in the Role that is used by amplify. We looked at and successfully established cross account access using the method described [here](https://docs.amplify.aws/cli/usage/iam) however we cannot change roles after we ran amplify init so the end result is that we are provisioning the amplify app under the aws account that holds the central cognito instance. This issue is also mentioned [here](https://github.com/aws-amplify/amplify-cli/issues/7008).**Describe the solution you'd like**A straightforward way or recommended best practices for such scenario. We are building 2 separate apps and have a multiple others lined up in the queue. It is very important for us to get this straight before we commit more to amplify as a framework for all our apps.**Describe alternatives you've considered**Few looked at a few workarounds:1) We created a separate cognito pool and imported the the main cognito account through OIDC. - In this case login works but the user records are \"duplicated\" in the pools created by the amplify apps. - The access token that is issued for the apps does not belong to our central identity because it is issued by the amplify pool.- We are limited to use federated login in our mobile apps.2) We set the `@auth` directive on our AppSync schema to `oidc`, obtaining the token directly from the main pool through federated login.- In this case we cannot use Datastore (or just couldn't get it to work)- Other resources that don't support OIDC access will be difficult to work with.- In general seems like amplify expects a user pool imported, otherwise it looses most of it's convenience features.3) Abandon amplify completely, configure AppSync with `oidc` and other resources \"manually\".- If we decide to use `oidc` for AppSync auth (as far as we see now) there isn't much that amplify brings to the table. We can just setup our backend \"manually\" and keep the apps lightweight.- Obviously this is a lot more work than the other options and we would not pursue it if possible**Additional context**We would really like to use amplify and we leverage all it has, this it is really frustrating that that we cannot bypass that one step of importing the cognito pool. If this feature is something that will not be supported, we would still be happy to receive some feedback/direction on our workarounds above.\n"
     ]
    }
   ],
   "source": [
    "js_issue = query_df(js_df, number=8108)[corpus_col].item()\n",
    "print(js_issue)\n",
    "js_issue_ast = BeautifulSoup(js_issue, 'html.parser')\n",
    "print(js_issue_ast)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
