{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update and import packages\n",
    "!pip install -Uqr requirements.txt\n",
    "\n",
    "# Basic packages\n",
    "import importlib\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from progress.bar import Bar\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data science/NLP packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# AWS packages\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "from sagemaker.sklearn import SKLearn\n",
    "import boto3\n",
    "\n",
    "# Local modules\n",
    "import model\n",
    "import train\n",
    "for m in [model, train]:\n",
    "    importlib.reload(m)\n",
    "\n",
    "from model import VectorSimilarity, get_fitted_model\n",
    "from train import combine_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'b' 'c']]\n",
      "[[ 2.  1. -1.]]\n",
      "Prediction took 0.000789642333984375 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['a', 'b', 'c']], dtype='<U1'), array([[ 2.,  1., -1.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on VectorSimilarity\n",
    "X = np.array(\n",
    "    [[0, 1],\n",
    "     [1, 0],\n",
    "     [-1, 0]])\n",
    "y = np.array(['a', 'b', 'c'])\n",
    "\n",
    "estimator = VectorSimilarity()\n",
    "estimator = estimator.fit(X, y)\n",
    "estimator.predict(np.array([1, 2]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Took 1.484374761581421 seconds\n",
      "[['a' 'c' 'b' 'd']\n",
      " ['b' 'c' 'a' 'd']\n",
      " ['c' 'b' 'a' 'd']\n",
      " ['d' 'c' 'b' 'a']]\n",
      "[[1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.         0.         0.        ]]\n",
      "Prediction took 0.0005955696105957031 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Basic pipeline setup\n",
    "basic_corpus = [\n",
    "    'Bees like to make honey',\n",
    "    'Bears like to eat honey',\n",
    "    'Bees don\\'t like bears',\n",
    "    'Humans are walking around the park'\n",
    "]\n",
    "basic_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "pipe = get_fitted_model(basic_corpus, basic_labels, lemmatize='custom')\n",
    "pred, score = pipe.predict(basic_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken after changing IAM users\n"
     ]
    }
   ],
   "source": [
    "# Train and infer on small Data Wrangler dataset\n",
    "print('broken after changing IAM users')\n",
    "# sess = sagemaker.Session()\n",
    "# bucket = sess.default_bucket()\n",
    "\n",
    "# chunksize = 1000\n",
    "# output_content_type = \"parquet\"\n",
    "# flow_export_id = f\"30-23-06-49-58efbaf1\"\n",
    "# flow_export_name = f\"flow-{flow_export_id}\"\n",
    "# s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "# s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "\n",
    "# dfs=[]\n",
    "# if output_content_type.upper() == \"CSV\":\n",
    "#     dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "# elif output_content_type.upper() == \"PARQUET\":\n",
    "#     dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "# else:\n",
    "#     print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "# wrangled_df = next(dfs)\n",
    "\n",
    "# X = wrangled_df['bodyText']\n",
    "# y = wrangled_df['url']\n",
    "# pipe.fit(X, y)\n",
    "\n",
    "# pred, score = pipe.predict(X[13:14])\n",
    "# print(list(y[13:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File helper functions\n",
    "import io\n",
    "def list_data_objs():\n",
    "    secret_name = \"SageMakerS3Access\"\n",
    "    region_name = \"us-west-2\"\n",
    "    bucket_name = 'amplifyobserverinsights-aoinsightslandingbucket29-5vcr471d4nm5'\n",
    "    bucket_subfolder = 'data/issues/'\n",
    "    \n",
    "#     secrets = boto3.client(\n",
    "#         service_name='secretsmanager',\n",
    "#         region_name=region_name\n",
    "#     )\n",
    "\n",
    "#     secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "#     secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "#     (access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "    data_obj_names = [key['Key'] for key in data_objects]\n",
    "#     data_obj_names = [f\"s3://{bucket_name}/{key['Key']}\" for key in data_objects]\n",
    "    \n",
    "    return data_obj_names\n",
    "\n",
    "\n",
    "def download_data(filename, data_obj_names):\n",
    "    dfs = []\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "    )\n",
    "    \n",
    "    with Bar(\n",
    "        message='Downloading parquets',\n",
    "        check_tty=False,\n",
    "        hide_cursor=False,\n",
    "        max=len(data_obj_names)\n",
    "    ) as bar:\n",
    "\n",
    "        for obj_name in data_obj_names:\n",
    "#             df = wr.s3.read_csv(obj_name)\n",
    "            obj = s3.get_object(Bucket='amplifyobserverinsights-aoinsightslandingbucket29-5vcr471d4nm5', Key=obj_name)\n",
    "            df = pd.read_parquet(io.BytesIO(obj['Body'].read()))\n",
    "            dfs.append(df)\n",
    "            bar.next()\n",
    "\n",
    "        bar.finish()\n",
    "        \n",
    "    df = combine_dfs(dfs)\n",
    "    df.to_csv(filename)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_data(filename, force_redownload=False):\n",
    "    start = time()\n",
    "    data = Path(filename)\n",
    "    \n",
    "    if data.is_file() and not force_redownload:\n",
    "        print('Deserializing data from', filename, '...')\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "    else:\n",
    "        data_obj_names = list_data_objs()\n",
    "        df = download_data(filename, data_obj_names[1:]) # TODO: this is because list data objs is returning an empty thing\n",
    "        \n",
    "    print('Took', time() - start, 'seconds')    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data helper functions\n",
    "def query_df(df, **kwargs):\n",
    "    query = True\n",
    "    for key, value in kwargs.items():\n",
    "        query &= (df[key] == value)\n",
    "        \n",
    "    result = df[query]\n",
    "    return result\n",
    "\n",
    "def compare_vecs():\n",
    "    print('unimplemented')\n",
    "\n",
    "def inspect_doc(vectorizer, doc, n_best=10):\n",
    "    if type(doc) == str:\n",
    "        doc = [doc]\n",
    "        \n",
    "    if len(doc) > 1:\n",
    "        raise ValueError('Only one document per call is supported')\n",
    "\n",
    "    vocab = np.array(vectorizer.get_feature_names(), ndmin=2)\n",
    "    weights = vectorizer.transform(doc).toarray()\n",
    "    weights_desc_args = np.flip(weights.argsort())\n",
    "    words_desc = np.take_along_axis(vocab, weights_desc_args, axis=1)\n",
    "    weights_desc = np.take_along_axis(weights, weights_desc_args, axis=1)\n",
    "    \n",
    "    print(words_desc[:, :n_best])\n",
    "    print(weights_desc[:, :n_best])\n",
    "    \n",
    "def get_weights(vectorizer, query, doc, n_best=10):\n",
    "    if type(doc) == str:\n",
    "        doc = [doc]\n",
    "        \n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "    tokens = tokenizer(query)\n",
    "    print(tokens)\n",
    "    vocab = np.array(vectorizer.get_feature_names(), ndmin=2)\n",
    "    indices = []\n",
    "    for tok in tokens:\n",
    "        indices.append(np.argwhere(vocab == tok)[0, 1])\n",
    "\n",
    "    indices = np.array(indices)\n",
    "    weights = vectorizer.transform(doc).toarray().flatten()\n",
    "    print(indices)\n",
    "    weights_desc = np.take_along_axis(weights, indices, axis=0)\n",
    "    print(weights_desc[:n_best])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::092109498566:role/service-role/AmazonSageMaker-ExecutionRole-20210728T153017\n",
      "us-west-2\n",
      "<botocore.client.SageMaker object at 0x7f17559ebc90>\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(role)\n",
    "region = boto3.Session().region_name\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "print(region)\n",
    "print(smclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading parquets |################################| 18/18\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.9556849002838135 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download and compile parquets\n",
    "iden = boto3.client('sts').get_caller_identity()\n",
    "# print(iden)\n",
    "df = get_data('training_data.csv', force_redownload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Took 4.7206878662109375e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training data\n",
    "corpus_col = 'title_body'\n",
    "label_col = 'url'\n",
    "\n",
    "begin_text = r'.*Describe the bug'\n",
    "mid_text = r'### Expected behavior|### Reproduction steps|\\r\\n*'\n",
    "end_text = r'### Code Snippet.*'\n",
    "begin_text_feat = r'.*Describe the feature you\\'d like to request'\n",
    "mid_text_feat = r'### Describe the solution you\\'d like|### Describe alternatives you\\'ve considered'\n",
    "end_text_feat = r'### Additional context.*'\n",
    "\n",
    "cases = [\n",
    "    begin_text,\n",
    "    mid_text,\n",
    "    end_text,\n",
    "    begin_text_feat,\n",
    "    mid_text_feat,\n",
    "    end_text_feat\n",
    "]\n",
    "pat_cases = '(' + '|'.join(cases) + ')'\n",
    "pat = re.compile(pat_cases, flags=(re.DOTALL | re.M))\n",
    "\n",
    "print('Preprocessing data...')\n",
    "start = time()\n",
    "\n",
    "# js_df = query_df(df, repo='amplify-js')\n",
    "js_df = df\n",
    "# clean_template = lambda text : re.sub(pat, '', text)\n",
    "# js_df[corpus_col] = js_df[corpus_col].apply(clean_template)\n",
    "\n",
    "print('Took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 76.53842520713806 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "small_df = js_df\n",
    "corpus = small_df[corpus_col]\n",
    "labels = small_df[label_col]\n",
    "\n",
    "pipe = get_fitted_model(corpus, labels, lemmatize='custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab words: 23734\n",
      "Available repos: ['amplify-flutter', 'community', 'docs', 'amplify-ios', 'amplify-console', 'amplify-observer', 'amplify-cli', 'aws-appsync-realtime-client-ios', 'aws-sdk-ios', 'amplify-ci-support', 'amplify-codegen', 'amplify-js-samples', 'aws-amplify.github.io', 'aws-sdk-android', 'amplify-ui', 'amplify-js', 'amplify-android', 'amplify-adminui']\n",
      "Small words in vocab:\n",
      " ['ad', 'al', 'au', 'az', 'bk', 'ca', 'cc', 'cd', 'cf', 'ci', 'cm', 'cs', 'cu', 'cv', 'db', 'dm', 'dy', 'ec', 'ed', 'em', 'er', 'es', 'fo', 'gb', 'gc', 'gi', 'gm', 'hl', 'hr', 'hz', 'ic', 'id', 'io', 'ip', 'iv', 'ki', 'km', 'kv', 'l', 'lf', 'lm', 'mb', 'mi', 'os', 'pc', 'pe', 'po', 'pr', 'r', 's', 'sc', 'sd', 'si', 'sl', 'sm', 'sn', 'ss', 'ti', 'tl', 'tn', 'tt', 'vi', 'w', 'x', 'zu']\n"
     ]
    }
   ],
   "source": [
    "# Model stats\n",
    "vocab = pipe['tfidfvectorizer'].get_feature_names()\n",
    "print('Number of vocab words:', len(vocab))\n",
    "repo_list = list(set(df['repo']))\n",
    "print('Available repos:', repo_list)\n",
    "small_words = [word for word in vocab if len(word) < 3]\n",
    "print('Small words in vocab:\\n', small_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    f.write(str(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pipe[0].build_tokenizer()\n",
    "print(tokenizer.custom)\n",
    "token = tokenizer('walk walks talk talks talking talked')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = query_df(js_df, number=8108)[corpus_col].item()\n",
    "inspect_doc(pipe[0], js_issue)\n",
    "get_weights(pipe[0], 'workarounds issued cognito pool', js_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SM estimator\n",
    "train_data = 's3://githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v/data'\n",
    "sklearn_estimator = SKLearn('train.py',\n",
    "                            role=sagemaker.get_execution_role(),\n",
    "                            instance_type='ml.m5.4xlarge',\n",
    "                            framework_version='0.23-1',\n",
    "                            hyperparameters = {'n-best': 10})\n",
    "sklearn_estimator.fit({'train': train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy SM predictor\n",
    "predictor = sklearn_estimator.deploy(instance_type='ml.m5.4xlarge',\n",
    "                                     initial_instance_count=1)\n",
    "data = ['password managers autofill']\n",
    "response = predictor.predict(data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('extra_tokens.txt', 'w') as f:\n",
    "#     f.write(str(set(vocab2) - set(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_mgr = query_df(js_df, number=5782)['bodyCleaned']\n",
    "inspect_doc(pipe[0], pw_mgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://github.com/aws-amplify/amplify-js/issues/8472'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/233'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/5782'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/8289'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4748'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3799'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/14'\n",
      "  'https://github.com/aws-amplify/aws-sdk-ios/issues/3076'\n",
      "  'https://github.com/aws-amplify/aws-sdk-ios/issues/313'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/6111']]\n",
      "[[0.7425792  0.33807384 0.31580219 0.29732867 0.29658787 0.29270244\n",
      "  0.28858939 0.2834566  0.26672109 0.26536774]]\n",
      "Prediction took 2.312330722808838 seconds\n"
     ]
    }
   ],
   "source": [
    "pw_mgr_query = ['AmplifySignIn component does not work with password managers or native browser autofill']\n",
    "pred, score = pipe.predict(pw_mgr_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = ['image file upload fail file size 5 mb']\n",
    "pipe.predict(js_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://github.com/aws-amplify/amplify-js/issues/4981'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/5576'\n",
      "  'https://github.com/aws-amplify/amplify-console/issues/1519'\n",
      "  'https://github.com/aws-amplify/amplify-cli/issues/1770'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3706'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/1646'\n",
      "  'https://github.com/aws-amplify/amplify-cli/issues/5260'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/515'\n",
      "  'https://github.com/aws-amplify/amplify-cli/issues/4822'\n",
      "  'https://github.com/aws-amplify/aws-sdk-android/issues/375']]\n",
      "[[0.72054029 0.41931634 0.33497204 0.33299807 0.31423847 0.28113276\n",
      "  0.27258603 0.26524243 0.26306481 0.26025255]]\n",
      "Prediction took 2.3095221519470215 seconds\n",
      "[['header' 'request' 'navegaor' 'server' 'end' 'proxy' 'standard' 'make'\n",
      "  'cors' 'saw']]\n",
      "[[0.61793193 0.41295811 0.21754669 0.20195419 0.18303126 0.14329843\n",
      "  0.13805923 0.13789834 0.1327474  0.12866307]]\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['additionalHeaders param is never passed to underlying function -- How do I access the current request headers or set them per request']\n",
    "pred, score = pipe.predict(js_issue)\n",
    "inspect_doc(pipe[0], query_df(small_df, repo='amplify-console', number=1519)['title_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = query_df(js_df, number=8108)[corpus_col].item()\n",
    "print(js_issue)\n",
    "js_issue_ast = BeautifulSoup(js_issue, 'html.parser')\n",
    "print(js_issue_ast)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
