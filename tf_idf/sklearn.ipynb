{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update and import packages\n",
    "!pip install -Uqr requirements.txt\n",
    "\n",
    "# Basic packages\n",
    "import importlib\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from progress.bar import Bar\n",
    "import json\n",
    "import re\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data science/NLP packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "nltk.download([\n",
    "    'punkt',\n",
    "    'wordnet',\n",
    "    'tagsets',\n",
    "    'averaged_perceptron_tagger'\n",
    "    ], quiet=True\n",
    ")\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# AWS packages\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "from sagemaker.sklearn import SKLearn\n",
    "import boto3\n",
    "\n",
    "# Local modules\n",
    "import vector_similarity\n",
    "importlib.reload(vector_similarity)\n",
    "from vector_similarity import VectorSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['a', 'b', 'c']], dtype='<U1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on VectorSimilarity\n",
    "X = np.array(\n",
    "    [[0, 1],\n",
    "     [1, 0],\n",
    "     [-1, 0]])\n",
    "y = np.array(['a', 'b', 'c'])\n",
    "\n",
    "estimator = VectorSimilarity()\n",
    "estimator = estimator.fit(X, y)\n",
    "estimator.predict(np.array([1, 2]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'b' 'c' 'd']\n",
      " ['b' 'a' 'c' 'd']\n",
      " ['c' 'b' 'a' 'd']\n",
      " ['d' 'c' 'b' 'a']]\n",
      "[[1.         0.50443175 0.3494023  0.        ]\n",
      " [1.         0.50443175 0.3494023  0.        ]\n",
      " [1.         0.3494023  0.3494023  0.        ]\n",
      " [1.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Basic pipeline setup\n",
    "basic_corpus = [\n",
    "    'Bees like to make honey',\n",
    "    'Bears like to eat honey',\n",
    "    'Bees don\\'t like bears',\n",
    "    'Humans are walking around the park'\n",
    "]\n",
    "basic_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    VectorSimilarity()\n",
    ")\n",
    "# Add the predict_score() function from VectorSimilarity - inelegant, but gets\n",
    "# the job done\n",
    "pipe.predict_score = lambda x : pipe[1].predict_score(pipe[0].transform(x))\n",
    "\n",
    "pipe.fit(basic_corpus, basic_labels)\n",
    "pred, score = pipe.predict_score(basic_corpus)\n",
    "print(pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://github.com/aws-amplify/amplify-adminui/issues/12'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/21'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/67'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/82'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/41'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/28'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/85'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/45'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/35'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/33']]\n",
      "[[1.         0.6949403  0.23125501 0.13250384 0.12011294 0.12004747\n",
      "  0.11126224 0.08492276 0.08372554 0.07978957]]\n",
      "['https://github.com/aws-amplify/amplify-adminui/issues/12']\n"
     ]
    }
   ],
   "source": [
    "# Train and infer on small Data Wrangler dataset\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "chunksize = 1000\n",
    "output_content_type = \"parquet\"\n",
    "flow_export_id = f\"30-23-06-49-58efbaf1\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\"\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "elif output_content_type.upper() == \"PARQUET\":\n",
    "    dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "wrangled_df = next(dfs)\n",
    "\n",
    "X = wrangled_df['bodyText']\n",
    "y = wrangled_df['url']\n",
    "pipe.fit(X, y)\n",
    "\n",
    "pred, score = pipe.predict_score(X[13:14])\n",
    "print(pred)\n",
    "print(score)\n",
    "print(list(y[13:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data parquets\n",
    "secret_name = \"SageMakerS3Access\"\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "secrets = boto3.client(\n",
    "    service_name='secretsmanager',\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "(access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "bucket_name = 'githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v'\n",
    "bucket_subfolder = 'data/'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "data_obj_names = [key['Key'] for key in data_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File helper functions\n",
    "def list_data_objs():\n",
    "    secret_name = \"SageMakerS3Access\"\n",
    "    region_name = \"us-west-2\"\n",
    "    bucket_name = 'githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v'\n",
    "    bucket_subfolder = 'data/'\n",
    "    \n",
    "    secrets = boto3.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "    secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "    (access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "    data_obj_names = [f\"s3://{bucket_name}/{key['Key']}\" for key in data_objects]\n",
    "    \n",
    "    return data_obj_names\n",
    "\n",
    "def combine_dfs(dfs):\n",
    "    df = pd.concat(\n",
    "            dfs,\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Clear empty values and reset indices\n",
    "    df = df[(not isinstance(df.bodyText, str)) and (df.bodyText != '')]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def download_data(filename, data_obj_names):\n",
    "    dfs = []\n",
    "    \n",
    "    with Bar(\n",
    "        message='Downloading parquets',\n",
    "        check_tty=False,\n",
    "        hide_cursor=False,\n",
    "        max=len(data_obj_names)\n",
    "    ) as bar:\n",
    "\n",
    "        for obj_name in data_obj_names:\n",
    "            df = wr.s3.read_parquet(obj_name)\n",
    "            dfs.append(df)\n",
    "            bar.next()\n",
    "\n",
    "        bar.finish()\n",
    "        \n",
    "    df = combine_dfs(dfs)\n",
    "    return df\n",
    "\n",
    "def get_data(filename, force_redownload=False):\n",
    "    start = time()\n",
    "    data = Path(filename)\n",
    "    \n",
    "    if data.is_file() and not force_redownload:\n",
    "        print('Deserializing data from', filename, '...')\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "    else:\n",
    "        data_obj_names = list_data_objs()\n",
    "        df = download_data(filename, data_obj_names)\n",
    "        \n",
    "    print('Took', time() - start, 'seconds')    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data helper functions\n",
    "def query_df(df, **kwargs):\n",
    "    query = True\n",
    "    for key, value in kwargs.items():\n",
    "        query &= (df[key] == value)\n",
    "        \n",
    "    result = df[query]\n",
    "    return result\n",
    "\n",
    "def compare_vecs():\n",
    "    print('unimplemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model helper functions\n",
    "def infer(pipe, text, show_score=False):\n",
    "    print('Inferring on the query:', text)\n",
    "    start = time()\n",
    "    if type(text) == str:\n",
    "        text = list(text)\n",
    "        \n",
    "    print(pipe.predict(text))\n",
    "    \n",
    "    if show_score:\n",
    "        print(pipe.score(text))\n",
    "    print('Took', time() - start, 'seconds')\n",
    "    \n",
    "class LemmaTokenizer:\n",
    "    def __init__(self, custom=False):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.custom = custom\n",
    "    def __call__(self, doc):\n",
    "        if self.custom:\n",
    "            # Find alphabetical tokens at least 3 chars long\n",
    "            tokens = re.findall(r\"(?u)\\b\\w\\w+\\b\", doc)\n",
    "            tokens = [word for word in tokens if len(word) >=3]\n",
    "            \n",
    "            # Only use verb/noun tokens\n",
    "            tags = nltk.pos_tag(tokens)\n",
    "            tokens = [word for word, tag in tags if tag[0] in ['V', 'N']]\n",
    "        \n",
    "        else:\n",
    "            tokens = word_tokenize(doc)\n",
    "        \n",
    "        lemmatized_tokens = [self.wnl.lemmatize(t) for t in tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "def get_trained_model(corpus, labels, lemmatize='default'):\n",
    "    print('Training model...')\n",
    "    start = time()\n",
    "    lemmatize = lemmatize.lower()\n",
    "\n",
    "    # Set lemmatization, if any\n",
    "    if lemmatize == 'default':\n",
    "        vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
    "    elif lemmatize == 'custom':\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer=LemmaTokenizer(custom=True)\n",
    "        )\n",
    "    elif lemmatize == 'none':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    else:\n",
    "        raise ValueError('lemmatize must be {default, custom, none}')\n",
    "    \n",
    "    # Create and train pipeline\n",
    "    pipe = make_pipeline(\n",
    "        vectorizer,\n",
    "        VectorSimilarity()\n",
    "    )\n",
    "    pipe.fit(corpus, labels)\n",
    "    \n",
    "    print('Took', time() - start, 'seconds')\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "def inspect_corpus():\n",
    "    print('unimplemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deserializing data from training_data.csv ...\n",
      "Took 3.805649518966675 seconds\n"
     ]
    }
   ],
   "source": [
    "# Download and compile parquets\n",
    "df = get_data('training_data.csv', force_redownload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Took 301.87840962409973 seconds\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training data\n",
    "corpus_col = 'bodyCleaned'\n",
    "label_col = 'url'\n",
    "\n",
    "begin_text = r'.*Describe the bug'\n",
    "mid_text = r'### Expected behavior|### Reproduction steps|\\r\\n*'\n",
    "end_text = r'### Code Snippet.*'\n",
    "begin_text_feat = r'.*Describe the feature you\\'d like to request'\n",
    "mid_text_feat = r'### Describe the solution you\\'d like|### Describe alternatives you\\'ve considered'\n",
    "end_text_feat = r'### Additional context.*'\n",
    "\n",
    "cases = [\n",
    "    begin_text,\n",
    "    mid_text,\n",
    "    end_text,\n",
    "    begin_text_feat,\n",
    "    mid_text_feat,\n",
    "    end_text_feat\n",
    "]\n",
    "pat_cases = '(' + '|'.join(cases) + ')'\n",
    "pat = re.compile(pat_cases, flags=(re.DOTALL | re.M))\n",
    "\n",
    "print('Preprocessing data...')\n",
    "start = time()\n",
    "\n",
    "# js_df = query_df(df, repository='amplify-js')\n",
    "js_df = df\n",
    "clean_template = lambda text : re.sub(pat, '', text)\n",
    "js_df[corpus_col] = js_df['title'] + ' ' + js_df['body'].apply(clean_template)\n",
    "\n",
    "print('Took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Took 156.8879110813141 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model (All default)\n",
    "# start = time()\n",
    "\n",
    "small_df = df\n",
    "corpus = small_df[corpus_col]\n",
    "labels = small_df[label_col]\n",
    "\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(\n",
    "#         tokenizer=LemmaTokenizer(),\n",
    "# #         stop_words='english'\n",
    "#     ),\n",
    "#     VectorSimilarity()\n",
    "# )\n",
    "# pipe.fit(corpus, labels)\n",
    "pipe = get_trained_model(corpus, labels, lemmatize='custom')\n",
    "# print('Training took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab words: 65345\n",
      "Available repos: ['aws-sdk-ios', 'amplify-js', 'docs', 'amplify-adminui', 'amplify-js-samples', 'amplify-console', 'amplify-codegen', 'aws-sdk-android', 'amplify-ci-support', 'amplify-ios', 'amplify-flutter', 'amplify-cli', 'amplify-android']\n",
      "Small words in vocab:\n",
      " ['ad', 'al', 'an', 'as', 'at', 'az', 'bk', 'ca', 'cc', 'cd', 'ce', 'cf', 'ci', 'cl', 'cm', 'co', 'cs', 'ct', 'cu', 'cv', 'db', 'dd', 'de', 'dm', 'do', 'dy', 'eb', 'ec', 'ed', 'em', 'er', 'es', 'fo', 'gb', 'gc', 'gi', 'gm', 'go', 'ha', 'hl', 'ho', 'hr', 'i', 'ic', 'id', 'il', 'in', 'io', 'ip', 'iv', 'j', 'km', 'kt', 'kv', 'la', 'le', 'lf', 'lh', 'li', 'lm', 'm1', 'mb', 'mi', 'nd', 'ni', 'nt', 'nv', 'ob', 'or', 'os', 'pa', 'pi', 'po', 'pr', 'r', 're', 'rh', 's', 'sc', 'si', 'sl', 'sm', 'sn', 'so', 'ss', 'te', 'tl', 'tn', 'tt', 'us', 'vi', 'w', 'wa', 'x']\n"
     ]
    }
   ],
   "source": [
    "# Model stats\n",
    "vocab = pipe['tfidfvectorizer'].get_feature_names()\n",
    "print('Number of vocab words:', len(vocab))\n",
    "repo_list = list(set(df['repository']))\n",
    "print('Available repos:', repo_list)\n",
    "small_words = [word for word in vocab if len(word) < 3]\n",
    "print('Small words in vocab:\\n', small_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-27 18:48:51 Starting - Starting the training job...\n",
      "2021-07-27 18:49:14 Starting - Launching requested ML instancesProfilerReport-1627411730: InProgress\n",
      "......\n",
      "2021-07-27 18:50:15 Starting - Preparing the instances for training......\n",
      "2021-07-27 18:51:20 Downloading - Downloading input data...\n",
      "2021-07-27 18:51:47 Training - Downloading the training image...\n",
      "2021-07-27 18:52:18 Uploading - Uploading generated training model\n",
      "2021-07-27 18:52:18 Failed - Training job failed\n",
      "ProfilerReport-1627411730: Stopping\n",
      "\u001b[34m2021-07-27 18:52:03,674 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2021-07-27 18:52:03,676 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-07-27 18:52:03,685 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-07-27 18:52:03,922 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-07-27 18:52:05,348 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-07-27 18:52:05,359 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-07-27 18:52:05,368 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"n-best\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2021-07-27-18-48-50-724\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-876895323997/sagemaker-scikit-learn-2021-07-27-18-48-50-724/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_tfidf\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_tfidf.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"n-best\":10}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_tfidf.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_tfidf\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-876895323997/sagemaker-scikit-learn-2021-07-27-18-48-50-724/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"n-best\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2021-07-27-18-48-50-724\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-876895323997/sagemaker-scikit-learn-2021-07-27-18-48-50-724/source/sourcedir.tar.gz\",\"module_name\":\"train_tfidf\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_tfidf.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--n-best\",\"10\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_N-BEST=10\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python train_tfidf.py --n-best 10\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train_tfidf.py\", line 27, in <module>\n",
      "    dfs = [pd.read_parquet(file, engine='pyarrow') for file in files]\n",
      "  File \"train_tfidf.py\", line 27, in <listcomp>\n",
      "    dfs = [pd.read_parquet(file, engine='pyarrow') for file in files]\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py\", line 317, in read_parquet\n",
      "    return impl.read(path, columns=columns, **kwargs)\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/pandas/io/parquet.py\", line 142, in read\n",
      "    path, columns=columns, filesystem=fs, **kwargs\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1281, in read_table\n",
      "    use_pandas_metadata=use_pandas_metadata)\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1137, in read\n",
      "    use_pandas_metadata=use_pandas_metadata)\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/pyarrow/parquet.py\", line 605, in read\n",
      "    table = reader.read(**options)\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/pyarrow/parquet.py\", line 253, in read\n",
      "    use_threads=use_threads)\n",
      "  File \"pyarrow/_parquet.pyx\", line 1136, in pyarrow._parquet.ParquetReader.read_all\n",
      "  File \"pyarrow/error.pxi\", line 105, in pyarrow.lib.check_status\u001b[0m\n",
      "\u001b[34mpyarrow.lib.ArrowNotImplementedError: Reading lists of structs from Parquet files not yet supported: array: list<element: struct<__typename: string, id: string, resourcePath: string, body: string, bodyHTML: string, bodyText: string, createdAt: string, databaseId: int32, isMinimized: bool, lastEditedAt: string, minimizedReason: string, publishedAt: string, updatedAt: string, url: string, authorAssociation: string, author: struct<__typename: string, login: string, resourcePath: string, id: string>, issueId: string, prId: string>>\u001b[0m\n",
      "\u001b[34m2021-07-27 18:52:06,600 sagemaker-containers ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2021-07-27 18:52:06,601 sagemaker-containers ERROR    framework error: \u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_trainer.py\", line 84, in train\n",
      "    entrypoint()\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/training.py\", line 39, in main\n",
      "    train(environment.Environment())\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/training.py\", line 35, in train\n",
      "    runner_type=runner.ProcessRunnerType)\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/entry_point.py\", line 100, in run\n",
      "    wait, capture_error\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/process.py\", line 161, in run\n",
      "    cwd=environment.code_dir,\n",
      "  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/process.py\", line 81, in check_error\n",
      "    raise error_class(return_code=return_code, cmd=\" \".join(cmd), output=stderr)\u001b[0m\n",
      "\u001b[34msagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/miniconda3/bin/python train_tfidf.py --n-best 10\"\n",
      "\u001b[0m\n",
      "\u001b[34mExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/miniconda3/bin/python train_tfidf.py --n-best 10\"\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job sagemaker-scikit-learn-2021-07-27-18-48-50-724: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_trainer.py\", line 84, in train\n    entrypoint()\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/training.py\", line 39, in main\n    train(environment.Environment())\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/training.py\", line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/entry_point.py\", line 100, in run\n    wait, capture_error\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/process.py\", line 161, in run\n    cwd=environment.code_dir,\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/process.py\", line 81, in check_error\n    raise error_class(return_code=return_code, cmd=\" \".join(cmd), output=stderr)\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dcfbe2b42f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mframework_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0.23-1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             hyperparameters = {'n-best': 10})\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msklearn_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3660\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3662\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3218\u001b[0m                 ),\n\u001b[1;32m   3219\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3220\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3221\u001b[0m             )\n\u001b[1;32m   3222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job sagemaker-scikit-learn-2021-07-27-18-48-50-724: Failed. Reason: AlgorithmError: framework error: \nTraceback (most recent call last):\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_containers/_trainer.py\", line 84, in train\n    entrypoint()\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/training.py\", line 39, in main\n    train(environment.Environment())\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_sklearn_container/training.py\", line 35, in train\n    runner_type=runner.ProcessRunnerType)\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/entry_point.py\", line 100, in run\n    wait, capture_error\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/process.py\", line 161, in run\n    cwd=environment.code_dir,\n  File \"/miniconda3/lib/python3.7/site-packages/sagemaker_training/process.py\", line 81, in check_error\n    raise error_class(return_code=return_code, cmd=\" \".join(cmd), output=stderr)\nsagemaker_training.errors.ExecuteUserScriptError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python "
     ]
    }
   ],
   "source": [
    "train_data = 's3://githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v/data'\n",
    "sklearn_estimator = SKLearn('train_tfidf.py',\n",
    "                            role=sagemaker.get_execution_role(),\n",
    "                            instance_type='ml.m5.4xlarge',\n",
    "                            framework_version='0.23-1',\n",
    "                            hyperparameters = {'n-best': 10})\n",
    "sklearn_estimator.fit({'train': train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn_estimator.deploy(instance_type='ml.m5.4xlarge',\n",
    "                                     initial_instance_count=1)\n",
    "data = ['password managers autofill']\n",
    "response = predictor.predict(data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('extra_tokens.txt', 'w') as f:\n",
    "#     f.write(str(set(vocab2) - set(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['Password managers autofill']\n",
      "[['https://github.com/aws-amplify/amplify-js/issues/8472'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4748'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3799'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/8289'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/5782'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7919'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3522'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/14'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/233'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2479']]\n",
      "[[0.61543985 0.45566849 0.40671398 0.39220031 0.30809485 0.29292973\n",
      "  0.26983688 0.26227048 0.26008897 0.25457087]]\n",
      "Took 21.20275568962097 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['Password managers autofill']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['image file upload fail file size 5 mb']\n",
      "[['https://github.com/aws-amplify/amplify-cli/issues/1525'\n",
      "  'https://github.com/aws-amplify/amplify-console/issues/604'\n",
      "  'https://github.com/aws-amplify/amplify-cli/issues/7434'\n",
      "  'https://github.com/aws-amplify/amplify-cli/issues/6008'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2977'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/125'\n",
      "  'https://github.com/aws-amplify/docs/issues/3243'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/6419'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3016'\n",
      "  'https://github.com/aws-amplify/docs/issues/2250']]\n",
      "[[0.47011621 0.41911752 0.41036934 0.39712141 0.32038145 0.31243851\n",
      "  0.30829788 0.29588254 0.28722115 0.28440392]]\n",
      "Took 21.234702110290527 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['image file upload fail file size 5 mb']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['user endpoint disappear pinpoint']\n",
      "[['https://github.com/aws-amplify/aws-sdk-ios/issues/1212'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/6896'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3819'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4573'\n",
      "  'https://github.com/aws-amplify/aws-sdk-android/issues/1927'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/92'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7675'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4712'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4529'\n",
      "  'https://github.com/aws-amplify/amplify-cli/issues/5204']]\n",
      "[[0.34888384 0.3452026  0.34273862 0.33471597 0.32414749 0.31828594\n",
      "  0.31434318 0.30612801 0.30452039 0.29743369]]\n",
      "Took 21.23546576499939 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['user endpoint disappear pinpoint']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect to External Cognito Account **Is your feature request related to a problem? Please describe.**We are building multiple apps in a multi-account enterprise AWS environment and we're trying to consume a central cognito pool that is under a different account other than the amplify apps (each amplify app is under it's own account following best practices). Hence when we try to import the cognito pool for auth the IAM user cannot see that in the Role that is used by amplify. We looked at and successfully established cross account access using the method described [here](https://docs.amplify.aws/cli/usage/iam) however we cannot change roles after we ran amplify init so the end result is that we are provisioning the amplify app under the aws account that holds the central cognito instance. This issue is also mentioned [here](https://github.com/aws-amplify/amplify-cli/issues/7008).**Describe the solution you'd like**A straightforward way or recommended best practices for such scenario. We are building 2 separate apps and have a multiple others lined up in the queue. It is very important for us to get this straight before we commit more to amplify as a framework for all our apps.**Describe alternatives you've considered**Few looked at a few workarounds:1) We created a separate cognito pool and imported the the main cognito account through OIDC. - In this case login works but the user records are \"duplicated\" in the pools created by the amplify apps. - The access token that is issued for the apps does not belong to our central identity because it is issued by the amplify pool.- We are limited to use federated login in our mobile apps.2) We set the `@auth` directive on our AppSync schema to `oidc`, obtaining the token directly from the main pool through federated login.- In this case we cannot use Datastore (or just couldn't get it to work)- Other resources that don't support OIDC access will be difficult to work with.- In general seems like amplify expects a user pool imported, otherwise it looses most of it's convenience features.3) Abandon amplify completely, configure AppSync with `oidc` and other resources \"manually\".- If we decide to use `oidc` for AppSync auth (as far as we see now) there isn't much that amplify brings to the table. We can just setup our backend \"manually\" and keep the apps lightweight.- Obviously this is a lot more work than the other options and we would not pursue it if possible**Additional context**We would really like to use amplify and we leverage all it has, this it is really frustrating that that we cannot bypass that one step of importing the cognito pool. If this feature is something that will not be supported, we would still be happy to receive some feedback/direction on our workarounds above.\n",
      "Connect to External Cognito Account **Is your feature request related to a problem? Please describe.**We are building multiple apps in a multi-account enterprise AWS environment and we're trying to consume a central cognito pool that is under a different account other than the amplify apps (each amplify app is under it's own account following best practices). Hence when we try to import the cognito pool for auth the IAM user cannot see that in the Role that is used by amplify. We looked at and successfully established cross account access using the method described [here](https://docs.amplify.aws/cli/usage/iam) however we cannot change roles after we ran amplify init so the end result is that we are provisioning the amplify app under the aws account that holds the central cognito instance. This issue is also mentioned [here](https://github.com/aws-amplify/amplify-cli/issues/7008).**Describe the solution you'd like**A straightforward way or recommended best practices for such scenario. We are building 2 separate apps and have a multiple others lined up in the queue. It is very important for us to get this straight before we commit more to amplify as a framework for all our apps.**Describe alternatives you've considered**Few looked at a few workarounds:1) We created a separate cognito pool and imported the the main cognito account through OIDC. - In this case login works but the user records are \"duplicated\" in the pools created by the amplify apps. - The access token that is issued for the apps does not belong to our central identity because it is issued by the amplify pool.- We are limited to use federated login in our mobile apps.2) We set the `@auth` directive on our AppSync schema to `oidc`, obtaining the token directly from the main pool through federated login.- In this case we cannot use Datastore (or just couldn't get it to work)- Other resources that don't support OIDC access will be difficult to work with.- In general seems like amplify expects a user pool imported, otherwise it looses most of it's convenience features.3) Abandon amplify completely, configure AppSync with `oidc` and other resources \"manually\".- If we decide to use `oidc` for AppSync auth (as far as we see now) there isn't much that amplify brings to the table. We can just setup our backend \"manually\" and keep the apps lightweight.- Obviously this is a lot more work than the other options and we would not pursue it if possible**Additional context**We would really like to use amplify and we leverage all it has, this it is really frustrating that that we cannot bypass that one step of importing the cognito pool. If this feature is something that will not be supported, we would still be happy to receive some feedback/direction on our workarounds above.\n"
     ]
    }
   ],
   "source": [
    "js_issue = query_df(js_df, number=8108)[corpus_col].item()\n",
    "print(js_issue)\n",
    "js_issue_ast = BeautifulSoup(js_issue, 'html.parser')\n",
    "print(js_issue_ast)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
