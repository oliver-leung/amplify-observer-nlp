{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# Update and import packages\n",
    "!pip install -Uqr requirements.txt\n",
    "\n",
    "import importlib\n",
    "from time import time\n",
    "from progress.bar import Bar\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import vector_similarity\n",
    "importlib.reload(vector_similarity)\n",
    "from vector_similarity import VectorSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['a', 'b', 'c']], dtype='<U1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on VectorSimilarity\n",
    "# check_estimator(VectorSimilarity())\n",
    "X = np.array(\n",
    "    [[0, 1],\n",
    "     [1, 0],\n",
    "     [-1, 0]])\n",
    "y = np.array(['a', 'b', 'c'])\n",
    "\n",
    "estimator = VectorSimilarity()\n",
    "estimator = estimator.fit(X, y)\n",
    "estimator.predict(np.array([1, 2]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'b' 'c' 'd']\n",
      " ['b' 'a' 'c' 'd']\n",
      " ['c' 'b' 'a' 'd']\n",
      " ['d' 'c' 'b' 'a']]\n",
      "[[1.         0.50443175 0.3494023  0.        ]\n",
      " [1.         0.50443175 0.3494023  0.        ]\n",
      " [1.         0.3494023  0.3494023  0.        ]\n",
      " [1.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Basic pipeline setup\n",
    "basic_corpus = [\n",
    "    'Bees like to make honey',\n",
    "    'Bears like to eat honey',\n",
    "    'Bees don\\'t like bears',\n",
    "    'Humans are walking around the park'\n",
    "]\n",
    "basic_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    VectorSimilarity()\n",
    ")\n",
    "pipe.fit(basic_corpus, basic_labels)\n",
    "print(pipe.predict(basic_corpus))\n",
    "print(pipe.score(basic_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https://github.com/aws-amplify/amplify-adminui/issues/12'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/21'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/67'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/82'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/41'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/28'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/85'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/45'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/35'\n",
      "  'https://github.com/aws-amplify/amplify-adminui/issues/33']]\n",
      "[[1.         0.6949403  0.23125501 0.13250384 0.12011294 0.12004747\n",
      "  0.11126224 0.08492276 0.08372554 0.07978957]]\n",
      "['https://github.com/aws-amplify/amplify-adminui/issues/12']\n"
     ]
    }
   ],
   "source": [
    "# Train and infer on small Data Wrangler dataset\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "chunksize = 1000\n",
    "output_content_type = \"parquet\"\n",
    "flow_export_id = f\"30-23-06-49-58efbaf1\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\"\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "elif output_content_type.upper() == \"PARQUET\":\n",
    "    dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "wrangled_df = next(dfs)\n",
    "\n",
    "X = wrangled_df['bodyText']\n",
    "y = wrangled_df['url']\n",
    "pipe.fit(X, y)\n",
    "\n",
    "print(pipe.predict(X[13:14]))\n",
    "print(pipe.score(X[13:14]))\n",
    "print(list(y[13:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def query_df(df, **kwargs):\n",
    "    query = True\n",
    "    for key, value in kwargs.items():\n",
    "        query &= (df[key] == value)\n",
    "        \n",
    "    result = df[query]\n",
    "    return result\n",
    "\n",
    "def infer(pipe, text):\n",
    "    print('Inferring on the query:', text)\n",
    "    start = time()\n",
    "    if type(text) == str:\n",
    "        text = list(text)\n",
    "        \n",
    "    print(pipe.predict(text))\n",
    "    print(pipe.score(text))\n",
    "    print('Took', time() - start, 'seconds')\n",
    "    \n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "def trained_model(df):\n",
    "    corpus = df[corpus_col]\n",
    "    labels = df[label_col]\n",
    "\n",
    "    pipe = make_pipeline(\n",
    "        TfidfVectorizer(tokenizer=LemmaTokenizer()),\n",
    "    #     TfidfVectorizer(),\n",
    "        VectorSimilarity()\n",
    "    )\n",
    "    pipe.fit(corpus, labels)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data parquets\n",
    "secret_name = \"SageMakerS3Access\"\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "secrets = boto3.client(\n",
    "    service_name='secretsmanager',\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "(access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "bucket_name = 'githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v'\n",
    "bucket_subfolder = 'data/'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "data_obj_names = [key['Key'] for key in data_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading parquets |################################| 426/426ownloading parquets |##                              | 35/426"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 87.49258422851562 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and compile parquets\n",
    "dfs = []\n",
    "start_time = time()\n",
    "\n",
    "with Bar(\n",
    "    message='Downloading parquets',\n",
    "    check_tty=False,\n",
    "    hide_cursor=False,\n",
    "    max=len(data_obj_names)\n",
    ") as bar:\n",
    "    \n",
    "    for obj_name in data_obj_names:\n",
    "        full_obj_name = f\"s3://{bucket_name}/{obj_name}\"\n",
    "        df = wr.s3.read_parquet(full_obj_name)\n",
    "        dfs.append(df)\n",
    "        bar.next()\n",
    "        \n",
    "    bar.finish()\n",
    "\n",
    "print('Took', time() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training data\n",
    "corpus_col = 'bodyCleaned'\n",
    "label_col = 'url'\n",
    "\n",
    "df = pd.concat(\n",
    "    dfs,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Clear empty values and reset indices\n",
    "df = df[(not isinstance(df.bodyText, str)) and (df.bodyText != '')]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "begin_text = r'.*Describe the bug'\n",
    "mid_text = r'### Expected behavior|### Reproduction steps|\\r\\n*'\n",
    "end_text = r'### Code Snippet.*'\n",
    "begin_text_feat = r'.*Describe the feature you\\'d like to request'\n",
    "mid_text_feat = r'### Describe the solution you\\'d like|### Describe alternatives you\\'ve considered'\n",
    "end_text_feat = r'### Additional context.*'\n",
    "\n",
    "cases = [\n",
    "    begin_text,\n",
    "    mid_text,\n",
    "    end_text,\n",
    "    begin_text_feat,\n",
    "    mid_text_feat,\n",
    "    end_text_feat\n",
    "]\n",
    "pat_cases = '(' + '|'.join(cases) + ')'\n",
    "pat = re.compile(pat_cases, flags=(re.DOTALL | re.M))\n",
    "\n",
    "js_df = query_df(df, repository='amplify-js')\n",
    "clean_template = lambda text : re.sub(pat, '', text)\n",
    "js_df[corpus_col] = js_df['title'] + ' ' + js_df['body'].apply(clean_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize training data\n",
    "# df.to_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 15.939002275466919 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "start = time()\n",
    "\n",
    "small_df = js_df\n",
    "corpus = small_df[corpus_col]\n",
    "labels = small_df[label_col]\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(tokenizer=LemmaTokenizer()),\n",
    "#     TfidfVectorizer(),\n",
    "    VectorSimilarity()\n",
    ")\n",
    "pipe.fit(corpus, labels)\n",
    "\n",
    "print('Training took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab words: 40539\n",
      "Available repos: ['amplify-console', 'amplify-ci-support', 'amplify-adminui', 'amplify-ios', 'amplify-cli', 'amplify-codegen', 'amplify-js', 'aws-sdk-android', 'amplify-flutter', 'aws-sdk-ios', 'amplify-js-samples', 'docs', 'amplify-android']\n"
     ]
    }
   ],
   "source": [
    "# Model stats\n",
    "vocab2 = pipe['tfidfvectorizer'].get_feature_names()\n",
    "print('Number of vocab words:', len(vocab))\n",
    "repo_list = list(set(df['repository']))\n",
    "print('Available repos:', repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extra_tokens.txt', 'w') as f:\n",
    "    f.write(str(set(vocab2) - set(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['Password managers']\n",
      "[['https://github.com/aws-amplify/amplify-js/issues/8289'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/8472'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7919'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/14'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3522'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/5782'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/5915'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4170'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2383'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2895']]\n",
      "[[0.68070084 0.400885   0.26231051 0.23851386 0.22457498 0.21925387\n",
      "  0.20909575 0.18528949 0.18398587 0.17632967]]\n",
      "Took 2.92081880569458 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['Password managers']\n",
    "infer(pipe, js_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['image file upload fail file size 5 mb']\n",
      "[['https://github.com/aws-amplify/amplify-js/issues/7117'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7574'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/6419'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/125'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/6824'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/1673'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2965'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/5348'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7625'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2211']]\n",
      "[[0.34436212 0.29488971 0.27931818 0.27249194 0.25649529 0.24599132\n",
      "  0.23468639 0.22993672 0.2279019  0.22142294]]\n",
      "Took 2.990173816680908 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['image file upload fail file size 5 mb']\n",
    "infer(pipe, js_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring on the query: ['user endpoint disappear pinpoint']\n",
      "[['https://github.com/aws-amplify/amplify-js/issues/6896'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/7675'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/4529'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3886'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/62'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/8385'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3126'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/386'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/2057'\n",
      "  'https://github.com/aws-amplify/amplify-js/issues/3819']]\n",
      "[[0.25109051 0.23307928 0.23134839 0.22407729 0.20790471 0.20510456\n",
      "  0.20499026 0.19910681 0.19730286 0.19353353]]\n",
      "Took 2.806058645248413 seconds\n"
     ]
    }
   ],
   "source": [
    "js_issue = ['user endpoint disappear pinpoint']\n",
    "infer(pipe, js_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14739    Connect to External Cognito Account **Is your feature request related to a problem? Please describe.**We are building multiple apps in a multi-account enterprise AWS environment and we're trying to consume a central cognito pool that is under a different account other than the amplify apps (each amplify app is under it's own account following best practices). Hence when we try to import the cognito pool for auth the IAM user cannot see that in the Role that is used by amplify. We looked at and successfully established cross account access using the method described [here](https://docs.amplify.aws/cli/usage/iam) however we cannot change roles after we ran amplify init so the end result is that we are provisioning the amplify app under the aws account that holds the central cognito instance. This issue is also mentioned [here](https://github.com/aws-amplify/amplify-cli/issues/7008).**Describe the solution you'd like**A straightforward way or recommended best practices for such scenario. We are building 2 separate apps and have a multiple others lined up in the queue. It is very important for us to get this straight before we commit more to amplify as a framework for all our apps.**Describe alternatives you've considered**Few looked at a few workarounds:1) We created a separate cognito pool and imported the the main cognito account through OIDC. - In this case login works but the user records are \"duplicated\" in the pools created by the amplify apps. - The access token that is issued for the apps does not belong to our central identity because it is issued by the amplify pool.- We are limited to use federated login in our mobile apps.2) We set the `@auth` directive on our AppSync schema to `oidc`, obtaining the token directly from the main pool through federated login.- In this case we cannot use Datastore (or just couldn't get it to work)- Other resources that don't support OIDC access will be difficult to work with.- In general seems like amplify expects a user pool imported, otherwise it looses most of it's convenience features.3) Abandon amplify completely, configure AppSync with `oidc` and other resources \"manually\".- If we decide to use `oidc` for AppSync auth (as far as we see now) there isn't much that amplify brings to the table. We can just setup our backend \"manually\" and keep the apps lightweight.- Obviously this is a lot more work than the other options and we would not pursue it if possible**Additional context**We would really like to use amplify and we leverage all it has, this it is really frustrating that that we cannot bypass that one step of importing the cognito pool. If this feature is something that will not be supported, we would still be happy to receive some feedback/direction on our workarounds above.\n",
      "Name: bodyCleaned, dtype: string\n"
     ]
    }
   ],
   "source": [
    "print(query_df(js_df, number=8108)[corpus_col])"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
