{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.20.2 requires botocore==1.21.2, but you have botocore 1.21.9 which is incompatible.\n",
      "aiobotocore 1.3.3 requires botocore<1.20.107,>=1.20.106, but you have botocore 1.21.9 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update and import packages\n",
    "!pip install -Uqr requirements.txt\n",
    "\n",
    "# Basic packages\n",
    "import importlib\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from progress.bar import Bar\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Data science/NLP packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# AWS packages\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "from sagemaker.sklearn import SKLearn\n",
    "import boto3\n",
    "\n",
    "# Local modules\n",
    "import model\n",
    "import train\n",
    "for m in [model, train]:\n",
    "    importlib.reload(m)\n",
    "\n",
    "from model import VectorSimilarity, get_fitted_model\n",
    "from train import combine_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'b' 'c']]\n",
      "[[ 2.  1. -1.]]\n",
      "Prediction took 0.0009329319000244141 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([['a', 'b', 'c']], dtype='<U1'), array([[ 2.,  1., -1.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on VectorSimilarity\n",
    "X = np.array(\n",
    "    [[0, 1],\n",
    "     [1, 0],\n",
    "     [-1, 0]])\n",
    "y = np.array(['a', 'b', 'c'])\n",
    "\n",
    "estimator = VectorSimilarity()\n",
    "estimator = estimator.fit(X, y)\n",
    "estimator.predict(np.array([1, 2]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Took 0.14867162704467773 seconds\n",
      "[['a' 'c' 'b' 'd']\n",
      " ['b' 'c' 'a' 'd']\n",
      " ['c' 'b' 'a' 'd']\n",
      " ['d' 'c' 'b' 'a']]\n",
      "[[1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.         0.         0.        ]]\n",
      "Prediction took 0.0006616115570068359 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Basic pipeline setup\n",
    "basic_corpus = [\n",
    "    'Bees like to make honey',\n",
    "    'Bears like to eat honey',\n",
    "    'Bees don\\'t like bears',\n",
    "    'Humans are walking around the park'\n",
    "]\n",
    "basic_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "pipe = get_fitted_model(basic_corpus, basic_labels, lemmatize='custom')\n",
    "pred, score = pipe.predict(basic_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken after changing IAM users\n"
     ]
    }
   ],
   "source": [
    "# Train and infer on small Data Wrangler dataset\n",
    "print('broken after changing IAM users')\n",
    "# sess = sagemaker.Session()\n",
    "# bucket = sess.default_bucket()\n",
    "\n",
    "# chunksize = 1000\n",
    "# output_content_type = \"parquet\"\n",
    "# flow_export_id = f\"30-23-06-49-58efbaf1\"\n",
    "# flow_export_name = f\"flow-{flow_export_id}\"\n",
    "# s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "# s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "\n",
    "# dfs=[]\n",
    "# if output_content_type.upper() == \"CSV\":\n",
    "#     dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "# elif output_content_type.upper() == \"PARQUET\":\n",
    "#     dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "# else:\n",
    "#     print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "# wrangled_df = next(dfs)\n",
    "\n",
    "# X = wrangled_df['bodyText']\n",
    "# y = wrangled_df['url']\n",
    "# pipe.fit(X, y)\n",
    "\n",
    "# pred, score = pipe.predict(X[13:14])\n",
    "# print(list(y[13:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File helper functions\n",
    "def list_data_objs():\n",
    "    secret_name = \"SageMakerS3Access\"\n",
    "    region_name = \"us-west-2\"\n",
    "    bucket_name = 'amplifyobserverinsights-aoinsightslandingbucket29-5vcr471d4nm5'\n",
    "    bucket_subfolder = 'etl-job/csv/issues/'\n",
    "    \n",
    "#     secrets = boto3.client(\n",
    "#         service_name='secretsmanager',\n",
    "#         region_name=region_name\n",
    "#     )\n",
    "\n",
    "#     secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "#     secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "#     (access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "    data_obj_names = [key['Key'] for key in data_objects]\n",
    "#     data_obj_names = [f\"s3://{bucket_name}/{key['Key']}\" for key in data_objects]\n",
    "    \n",
    "    return data_obj_names\n",
    "\n",
    "\n",
    "def download_data(filename, data_obj_names):\n",
    "    dfs = []\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id='ASIARK4RHNDDM7HBALO6',\n",
    "        aws_secret_access_key='0NYAQYCHGuBMGcSB7RUlanweLiSuy9Cd5gLh65hZ',\n",
    "        aws_session_token='IQoJb3JpZ2luX2VjEHYaCXVzLWVhc3QtMSJGMEQCIFDeqQ1/6y/olFuEI+nEqLAevxXgN6ntyjUXM4LeYKilAiBZDQ7cqGBt/WvllNcl8MULxBBls65m6S9pN5iJFbOlASqeAgh/EAAaDDA5MjEwOTQ5ODU2NiIMYRfuYBP14uHT29oCKvsBYjNCkHJH4OLMzaQqczWQHj9HwmY4JjvOMcpJIT2rdKxXCGd7yzmKFCpEokPRQo4tAdkV1S2RG9ZUE0SmKxV1y4/2SNA1wo6ugfLMwTW6paKRFKRxpIHol1zJudRCJVARAhCDkmrKw3A2b9zoTqE1isB1KwqSGl7/Jl7G6wVJZEVHyyJG261WKpoJRyVG/OWtRDqUFLBr5PrIrGZnBa1dSr9Ly2iMZdC5ILLVhFIw6yV6WqSxkxrr7yzt2wGXyLAkpQxdPFJRm5o9UYnY305G6niRlGL/VBIWTsoQNqY1Pec5fUpZ8t+3CRHlF5kHBunpmGPEKwjUqQiOrEUwzqeHiAY6ngHHboiB7VRP0ubWRsB94M85AJzetQFhuVgJ+MYpGf/e2tKqtWtF71Xt0eneBfPftFNvKDzzzkC5PUGbOqmAnNgcjtHkyMPuAzjCQfo/V2+C5NFGaaRkD2zHAuM7QZPUvR65bxXYNExpoFvEMrh5kBtbZrIJKkpHl7q3SSpSerbkcQzMIVNs/N5XeGmNXAT9OEoum7lkCZpV7c5ZqFCNuA=='\n",
    "    )\n",
    "    \n",
    "    with Bar(\n",
    "        message='Downloading parquets',\n",
    "        check_tty=False,\n",
    "        hide_cursor=False,\n",
    "        max=len(data_obj_names)\n",
    "    ) as bar:\n",
    "\n",
    "        for obj_name in data_obj_names:\n",
    "#             df = wr.s3.read_csv(obj_name)\n",
    "            obj = s3.get_object(Bucket='amplifyobserverinsights-aoinsightslandingbucket29-5vcr471d4nm5', Key=obj_name)\n",
    "            df = pd.read_csv(obj['Body'])\n",
    "            dfs.append(df)\n",
    "            bar.next()\n",
    "\n",
    "        bar.finish()\n",
    "        \n",
    "    df = combine_dfs(dfs)\n",
    "    df.to_csv(filename)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_data(filename, force_redownload=False):\n",
    "    start = time()\n",
    "    data = Path(filename)\n",
    "    \n",
    "    if data.is_file() and not force_redownload:\n",
    "        print('Deserializing data from', filename, '...')\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "    else:\n",
    "        data_obj_names = list_data_objs()\n",
    "        df = download_data(filename, data_obj_names)\n",
    "        \n",
    "    print('Took', time() - start, 'seconds')    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data helper functions\n",
    "def query_df(df, **kwargs):\n",
    "    query = True\n",
    "    for key, value in kwargs.items():\n",
    "        query &= (df[key] == value)\n",
    "        \n",
    "    result = df[query]\n",
    "    return result\n",
    "\n",
    "def compare_vecs():\n",
    "    print('unimplemented')\n",
    "\n",
    "def inspect_doc(vectorizer, doc, n_best=10):\n",
    "    if type(doc) == str:\n",
    "        doc = [doc]\n",
    "        \n",
    "    if len(doc) > 1:\n",
    "        raise ValueError('Only one document per call is supported')\n",
    "\n",
    "    vocab = np.array(vectorizer.get_feature_names(), ndmin=2)\n",
    "    weights = vectorizer.transform(doc).toarray()\n",
    "    weights_desc_args = np.flip(weights.argsort())\n",
    "    words_desc = np.take_along_axis(vocab, weights_desc_args, axis=1)\n",
    "    weights_desc = np.take_along_axis(weights, weights_desc_args, axis=1)\n",
    "    \n",
    "    print(words_desc[:, :n_best])\n",
    "    print(weights_desc[:, :n_best])\n",
    "    \n",
    "def get_weights(vectorizer, query, doc, n_best=10):\n",
    "    if type(doc) == str:\n",
    "        doc = [doc]\n",
    "        \n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "    tokens = tokenizer(query)\n",
    "    print(tokens)\n",
    "    vocab = np.array(vectorizer.get_feature_names(), ndmin=2)\n",
    "    indices = []\n",
    "    for tok in tokens:\n",
    "        indices.append(np.argwhere(vocab == tok)[0, 1])\n",
    "\n",
    "    indices = np.array(indices)\n",
    "    weights = vectorizer.transform(doc).toarray().flatten()\n",
    "    print(indices)\n",
    "    weights_desc = np.take_along_axis(weights, indices, axis=0)\n",
    "    print(weights_desc[:n_best])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::092109498566:role/service-role/AmazonSageMaker-ExecutionRole-20210728T112747\n",
      "us-east-2\n",
      "<botocore.client.SageMaker object at 0x7f06e967d590>\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(role)\n",
    "region = boto3.Session().region_name\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "print(region)\n",
    "print(smclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UserId': 'AROARK4RHNDDGYKORNDXY:SageMaker', 'Account': '092109498566', 'Arn': 'arn:aws:sts::092109498566:assumed-role/AmazonSageMaker-ExecutionRole-20210728T112747/SageMaker', 'ResponseMetadata': {'RequestId': 'ecac9c63-b72e-44a8-8fe6-96d7cd2b569e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ecac9c63-b72e-44a8-8fe6-96d7cd2b569e', 'content-type': 'text/xml', 'content-length': '470', 'date': 'Wed, 28 Jul 2021 22:11:39 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading parquets |                                | 0/18\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDenied) when calling the GetObject operation: Access Denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-943354a3cd08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0miden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_caller_identity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_redownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-cfb8e2418e51>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(filename, force_redownload)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdata_obj_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_data_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_obj_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Took'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-cfb8e2418e51>\u001b[0m in \u001b[0;36mdownload_data\u001b[0;34m(filename, data_obj_names)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mobj_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_obj_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#             df = wr.s3.read_csv(obj_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'amplifyobserverinsights-aoinsightslandingbucket29-5vcr471d4nm5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the GetObject operation: Access Denied"
     ]
    }
   ],
   "source": [
    "# Download and compile parquets\n",
    "iden = boto3.client('sts').get_caller_identity()\n",
    "print(iden)\n",
    "df = get_data('training_data.csv', force_redownload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data\n",
    "corpus_col = 'bodyCleaned'\n",
    "label_col = 'url'\n",
    "\n",
    "begin_text = r'.*Describe the bug'\n",
    "mid_text = r'### Expected behavior|### Reproduction steps|\\r\\n*'\n",
    "end_text = r'### Code Snippet.*'\n",
    "begin_text_feat = r'.*Describe the feature you\\'d like to request'\n",
    "mid_text_feat = r'### Describe the solution you\\'d like|### Describe alternatives you\\'ve considered'\n",
    "end_text_feat = r'### Additional context.*'\n",
    "\n",
    "cases = [\n",
    "    begin_text,\n",
    "    mid_text,\n",
    "    end_text,\n",
    "    begin_text_feat,\n",
    "    mid_text_feat,\n",
    "    end_text_feat\n",
    "]\n",
    "pat_cases = '(' + '|'.join(cases) + ')'\n",
    "pat = re.compile(pat_cases, flags=(re.DOTALL | re.M))\n",
    "\n",
    "print('Preprocessing data...')\n",
    "start = time()\n",
    "\n",
    "js_df = query_df(df, repository='amplify-js')\n",
    "# js_df = df\n",
    "clean_template = lambda text : re.sub(pat, '', text)\n",
    "js_df[corpus_col] = js_df['title'] + ' ' + js_df['body'].apply(clean_template)\n",
    "\n",
    "print('Took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "small_df = js_df\n",
    "corpus = small_df[corpus_col]\n",
    "labels = small_df[label_col]\n",
    "\n",
    "pipe = get_fitted_model(corpus, labels, lemmatize='custom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model stats\n",
    "vocab = pipe['tfidfvectorizer'].get_feature_names()\n",
    "print('Number of vocab words:', len(vocab))\n",
    "repo_list = list(set(df['repository']))\n",
    "print('Available repos:', repo_list)\n",
    "small_words = [word for word in vocab if len(word) < 3]\n",
    "print('Small words in vocab:\\n', small_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    f.write(str(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pipe[0].build_tokenizer()\n",
    "print(tokenizer.custom)\n",
    "token = tokenizer('walk walks talk talks talking talked')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = query_df(js_df, number=8108)[corpus_col].item()\n",
    "inspect_doc(pipe[0], js_issue)\n",
    "get_weights(pipe[0], 'workarounds issued cognito pool', js_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SM estimator\n",
    "train_data = 's3://githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v/data'\n",
    "sklearn_estimator = SKLearn('train.py',\n",
    "                            role=sagemaker.get_execution_role(),\n",
    "                            instance_type='ml.m5.4xlarge',\n",
    "                            framework_version='0.23-1',\n",
    "                            hyperparameters = {'n-best': 10})\n",
    "sklearn_estimator.fit({'train': train_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy SM predictor\n",
    "predictor = sklearn_estimator.deploy(instance_type='ml.m5.4xlarge',\n",
    "                                     initial_instance_count=1)\n",
    "data = ['password managers autofill']\n",
    "response = predictor.predict(data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('extra_tokens.txt', 'w') as f:\n",
    "#     f.write(str(set(vocab2) - set(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_mgr = query_df(js_df, number=5782)['bodyCleaned']\n",
    "inspect_doc(pipe[0], pw_mgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_mgr_query = ['Password manager autofill']\n",
    "pipe.predict(pw_mgr_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = ['image file upload fail file size 5 mb']\n",
    "pipe.predict(js_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = ['additionalheaders param customize request headers']\n",
    "pipe.predict(js_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = query_df(js_df, number=8108)[corpus_col].item()\n",
    "print(js_issue)\n",
    "js_issue_ast = BeautifulSoup(js_issue, 'html.parser')\n",
    "print(js_issue_ast)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
