{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update and import packages\n",
    "!pip install -Uqr requirements.txt\n",
    "\n",
    "import importlib\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from progress.bar import Bar\n",
    "import json\n",
    "import re\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "nltk.download(\n",
    "    [\n",
    "        'punkt',\n",
    "        'wordnet',\n",
    "        'tagsets',\n",
    "        'averaged_perceptron_tagger'\n",
    "    ], quiet=True\n",
    ")\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "import vector_similarity\n",
    "importlib.reload(vector_similarity)\n",
    "from vector_similarity import VectorSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['a', 'b', 'c']], dtype='<U1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity checks on VectorSimilarity\n",
    "# check_estimator(VectorSimilarity())\n",
    "X = np.array(\n",
    "    [[0, 1],\n",
    "     [1, 0],\n",
    "     [-1, 0]])\n",
    "y = np.array(['a', 'b', 'c'])\n",
    "\n",
    "estimator = VectorSimilarity()\n",
    "estimator = estimator.fit(X, y)\n",
    "estimator.predict(np.array([1, 2]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'b' 'c' 'd']\n",
      " ['b' 'a' 'c' 'd']\n",
      " ['c' 'b' 'a' 'd']\n",
      " ['d' 'c' 'b' 'a']]\n",
      "[[1.         0.50443175 0.3494023  0.        ]\n",
      " [1.         0.50443175 0.3494023  0.        ]\n",
      " [1.         0.3494023  0.3494023  0.        ]\n",
      " [1.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Basic pipeline setup\n",
    "basic_corpus = [\n",
    "    'Bees like to make honey',\n",
    "    'Bears like to eat honey',\n",
    "    'Bees don\\'t like bears',\n",
    "    'Humans are walking around the park'\n",
    "]\n",
    "basic_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    VectorSimilarity()\n",
    ")\n",
    "# Add the predict_score() function from VectorSimilarity - inelegant, but gets\n",
    "# the job done\n",
    "pipe.predict_score = lambda x : pipe[1].predict_score(pipe[0].transform(x))\n",
    "\n",
    "pipe.fit(basic_corpus, basic_labels)\n",
    "pred, score = pipe.predict_score(basic_corpus)\n",
    "print(pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and infer on small Data Wrangler dataset\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "chunksize = 1000\n",
    "output_content_type = \"parquet\"\n",
    "flow_export_id = f\"30-23-06-49-58efbaf1\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\"\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{s3_output_prefix}\"\n",
    "\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    dfs = wr.s3.read_csv(s3_output_path, chunksize=chunksize)\n",
    "elif output_content_type.upper() == \"PARQUET\":\n",
    "    dfs = wr.s3.read_parquet(s3_output_path, chunked=chunksize)\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\") \n",
    "\n",
    "wrangled_df = next(dfs)\n",
    "\n",
    "X = wrangled_df['bodyText']\n",
    "y = wrangled_df['url']\n",
    "pipe.fit(X, y)\n",
    "\n",
    "pred, score = pipe.predict_score(X[13:14])\n",
    "print(pred)\n",
    "print(score)\n",
    "print(list(y[13:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data parquets\n",
    "secret_name = \"SageMakerS3Access\"\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "secrets = boto3.client(\n",
    "    service_name='secretsmanager',\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "(access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "bucket_name = 'githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v'\n",
    "bucket_subfolder = 'data/'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "data_obj_names = [key['Key'] for key in data_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File helper functions\n",
    "def list_data_objs():\n",
    "    secret_name = \"SageMakerS3Access\"\n",
    "    region_name = \"us-west-2\"\n",
    "    bucket_name = 'githubmachinelearningstack-rawdatabucket79e6ae92-dvgbsz21ce9v'\n",
    "    bucket_subfolder = 'data/'\n",
    "    \n",
    "    secrets = boto3.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    secrets_response = secrets.get_secret_value(SecretId=secret_name)\n",
    "    secrets_dict = json.loads(secrets_response['SecretString'])\n",
    "    (access_key, secret_key), = secrets_dict.items()\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    data_objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=bucket_subfolder)['Contents']\n",
    "    data_obj_names = [f\"s3://{bucket_name}/{key['Key']}\" for key in data_objects]\n",
    "    \n",
    "    return data_obj_names\n",
    "\n",
    "def combine_dfs(dfs):\n",
    "    df = pd.concat(\n",
    "            dfs,\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Clear empty values and reset indices\n",
    "    df = df[(not isinstance(df.bodyText, str)) and (df.bodyText != '')]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def download_data(filename, data_obj_names):\n",
    "    dfs = []\n",
    "    \n",
    "    with Bar(\n",
    "        message='Downloading parquets',\n",
    "        check_tty=False,\n",
    "        hide_cursor=False,\n",
    "        max=len(data_obj_names)\n",
    "    ) as bar:\n",
    "\n",
    "        for obj_name in data_obj_names:\n",
    "            df = wr.s3.read_parquet(obj_name)\n",
    "            dfs.append(df)\n",
    "            bar.next()\n",
    "\n",
    "        bar.finish()\n",
    "        \n",
    "    df = combine_dfs(dfs)\n",
    "    return df\n",
    "\n",
    "def get_data(filename, force_redownload=False):\n",
    "    start = time()\n",
    "    data = Path(filename)\n",
    "    \n",
    "    if data.is_file() and not force_redownload:\n",
    "        print('Deserializing data from', filename, '...')\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "    else:\n",
    "        data_obj_names = list_data_objs()\n",
    "        df = download_data(filename, data_obj_names)\n",
    "        \n",
    "    print('Took', time() - start, 'seconds')    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data helper functions\n",
    "def query_df(df, **kwargs):\n",
    "    query = True\n",
    "    for key, value in kwargs.items():\n",
    "        query &= (df[key] == value)\n",
    "        \n",
    "    result = df[query]\n",
    "    return result\n",
    "\n",
    "def compare_vecs():\n",
    "    print('unimplemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model helper functions\n",
    "def infer(pipe, text, show_score=False):\n",
    "    print('Inferring on the query:', text)\n",
    "    start = time()\n",
    "    if type(text) == str:\n",
    "        text = list(text)\n",
    "        \n",
    "    print(pipe.predict(text))\n",
    "    \n",
    "    if show_score:\n",
    "        print(pipe.score(text))\n",
    "    print('Took', time() - start, 'seconds')\n",
    "    \n",
    "class LemmaTokenizer:\n",
    "    def __init__(self, custom=False):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.custom = custom\n",
    "    def __call__(self, doc):\n",
    "        if self.custom:\n",
    "            # Find alphabetical tokens at least 3 chars long\n",
    "            tokens = re.findall(r\"(?u)\\b\\w\\w+\\b\", doc)\n",
    "            tokens = [word for word in tokens if len(word) >=3]\n",
    "            \n",
    "            # Only use verb/noun tokens\n",
    "            tags = nltk.pos_tag(tokens)\n",
    "            tokens = [word for word, tag in tags if tag[0] in ['V', 'N']]\n",
    "        \n",
    "        else:\n",
    "            tokens = word_tokenize(doc)\n",
    "        \n",
    "        lemmatized_tokens = [self.wnl.lemmatize(t) for t in tokens]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "def get_trained_model(corpus, labels, lemmatize='default'):\n",
    "    print('Training model...')\n",
    "    start = time()\n",
    "    lemmatize = lemmatize.lower()\n",
    "\n",
    "    # Set lemmatization, if any\n",
    "    if lemmatize == 'default':\n",
    "        vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
    "    elif lemmatize == 'custom':\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            tokenizer=LemmaTokenizer(custom=True)\n",
    "        )\n",
    "    elif lemmatize == 'none':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    else:\n",
    "        raise ValueError('lemmatize must be {default, custom, none}')\n",
    "    \n",
    "    # Create and train pipeline\n",
    "    pipe = make_pipeline(\n",
    "        vectorizer,\n",
    "        VectorSimilarity()\n",
    "    )\n",
    "    pipe.fit(corpus, labels)\n",
    "    \n",
    "    print('Took', time() - start, 'seconds')\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "def inspect_corpus():\n",
    "    print('unimplemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and compile parquets\n",
    "df = get_data('training_data.csv', force_redownload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess training data\n",
    "corpus_col = 'bodyCleaned'\n",
    "label_col = 'url'\n",
    "\n",
    "begin_text = r'.*Describe the bug'\n",
    "mid_text = r'### Expected behavior|### Reproduction steps|\\r\\n*'\n",
    "end_text = r'### Code Snippet.*'\n",
    "begin_text_feat = r'.*Describe the feature you\\'d like to request'\n",
    "mid_text_feat = r'### Describe the solution you\\'d like|### Describe alternatives you\\'ve considered'\n",
    "end_text_feat = r'### Additional context.*'\n",
    "\n",
    "cases = [\n",
    "    begin_text,\n",
    "    mid_text,\n",
    "    end_text,\n",
    "    begin_text_feat,\n",
    "    mid_text_feat,\n",
    "    end_text_feat\n",
    "]\n",
    "pat_cases = '(' + '|'.join(cases) + ')'\n",
    "pat = re.compile(pat_cases, flags=(re.DOTALL | re.M))\n",
    "\n",
    "print('Preprocessing data...')\n",
    "start = time()\n",
    "\n",
    "js_df = query_df(df, repository='amplify-js')\n",
    "clean_template = lambda text : re.sub(pat, '', text)\n",
    "js_df[corpus_col] = js_df['title'] + ' ' + js_df['body'].apply(clean_template)\n",
    "\n",
    "print('Took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (All default)\n",
    "# start = time()\n",
    "\n",
    "small_df = js_df\n",
    "corpus = small_df[corpus_col]\n",
    "labels = small_df[label_col]\n",
    "\n",
    "# pipe = make_pipeline(\n",
    "#     TfidfVectorizer(\n",
    "#         tokenizer=LemmaTokenizer(),\n",
    "# #         stop_words='english'\n",
    "#     ),\n",
    "#     VectorSimilarity()\n",
    "# )\n",
    "# pipe.fit(corpus, labels)\n",
    "pipe = get_trained_model(corpus, labels, lemmatize='custom')\n",
    "# print('Training took', time() - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model stats\n",
    "vocab = pipe['tfidfvectorizer'].get_feature_names()\n",
    "print('Number of vocab words:', len(vocab))\n",
    "repo_list = list(set(df['repository']))\n",
    "print('Available repos:', repo_list)\n",
    "small_words = [word for word in vocab if len(word) < 3]\n",
    "print('Small words in vocab:\\n', small_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('extra_tokens.txt', 'w') as f:\n",
    "#     f.write(str(set(vocab2) - set(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = ['Password managers']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = ['image file upload fail file size 5 mb']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = ['user endpoint disappear pinpoint']\n",
    "infer(pipe, js_issue, show_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_issue = query_df(js_df, number=8108)[corpus_col].item()\n",
    "print(js_issue)\n",
    "js_issue_ast = BeautifulSoup(js_issue, 'html.parser')\n",
    "print(js_issue_ast)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
