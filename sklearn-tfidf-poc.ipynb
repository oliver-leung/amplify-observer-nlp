{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TF-IDF Predictor with Scikit-learn: Proof of Concept\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency** (TF-IDF) is an information retrieval (IR) statistic used to determine how important a word is to a document. Simply put, we determine the TF-IDF of a word in a document by counting the occurrences of that word in the document, then dividing it by the number of documents in which it appears. Then, this process is repeated for each word in the document, resulting in a vector indicating the relevance of each word to that document. It's a simple, but effective IR method that was introduced in the 1980s, yet has stood the test of time.\n",
    "\n",
    "The bread and butter of IR is the process of turning arbitrary-length documents into fixed-length vectors (other such methods include word embeddings, encode-decode model, etc.). Once we have vectors that abstractly represent the semantics of documents, we can compare such vectors using metrics like cosine similarity, i.e. the dot product between two vectors. This principle then forms the basis of our project - *using TF-IDF, we map a small query and large documents into a high-dimensional vector space, then determine which documents are most relevant to the query*. In essence, it is a search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update and import dependencies\n",
    "!pip install -Uqr requirements.txt\n",
    "\n",
    "# Basic packages\n",
    "import importlib\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from progress.bar import Bar\n",
    "import io\n",
    "\n",
    "# Data science/NLP packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# AWS packages\n",
    "import boto3\n",
    "\n",
    "# Local modules\n",
    "import tfidf_predictor, utils_tfidf, train_tfidf\n",
    "for m in [tfidf_predictor, utils_tfidf, train_tfidf]:\n",
    "    importlib.reload(m)\n",
    "\n",
    "from tfidf_predictor import VectorSimilarity, TfidfPredictor\n",
    "from train_tfidf import combine_dfs\n",
    "from utils_tfidf import get_data, get_corpus_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending SKLearn: VectorSimilarity\n",
    "\n",
    "In order to perform pairwise comparisons across vectors and find the most similar pairs, we implemented the `VectorSimilarity` class, extending SKLearn's `Estimator` interface. The fitting process of `VectorSimilarity` takes two arrays:\n",
    "\n",
    "1. An array of float vectors, each with the same dimensionality\n",
    "1. An array of labels, which can have any type or shape.\n",
    "\n",
    "Note that both arrays must have equal length, and that the `i`th vector corresponds to the `i`th label for all `i`. Then, when we feed it an input vector, `VectorSimilarity` compares that vector against all of the vectors that it was fitted on, returning the corresponding labels of the `n_best` vectors. A \"score\" is also returned, which is just the dot product with each of the `n_best` vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar vectors:\n",
      " [['b' 'a' 'c' 'd']]\n",
      "Confidence scores:\n",
      " [[ 1.   0.5 -0.5 -1. ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visual representation of vector locations\n",
    "\n",
    "       /\\\n",
    "        b  ?\n",
    "        |\n",
    "        |\n",
    "< c --------- a >\n",
    "        |\n",
    "        |\n",
    "        d\n",
    "       \\/\n",
    "\"\"\"\n",
    "X = np.array(\n",
    "    [[1, 0],\n",
    "     [0, 1],\n",
    "     [-1, 0],\n",
    "     [0, -1]]\n",
    ")\n",
    "y = np.array(['a', 'b', 'c', 'd'])\n",
    "\n",
    "sim_estimator = VectorSimilarity(n_best=10)\n",
    "sim_estimator = sim_estimator.fit(X, y)\n",
    "\n",
    "vec_input = np.array([0.5, 1]).reshape(1, -1) # Shape needs to be (1, n)\n",
    "pred, score = sim_estimator.predict(vec_input)\n",
    "print('Most similar vectors:\\n', pred)\n",
    "print('Confidence scores:\\n', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending SKLearn: TfidfPredictor\n",
    "\n",
    "To make similarity predictions on natural language, we use the above `VectorSimilarity` class in tandem with SKLearn's `TfidfVectorizer`, tying the two using SKLearn's `Pipeline` interface. We then wrapped the pipeline in another custom `Estimator`, `TfidfPredictor`, in order to have more control over the inputs and outputs of the model.\n",
    "\n",
    "The `TfidfVectorizer` takes in natural language documents and performs the following transformations:\n",
    "\n",
    "1. The document is separated into word tokens that are alphabetical and at least 3 characters long.\n",
    "1. Stop words such as articles, prepositions, and pronouns are removed.\n",
    "1. Lemmatization is applied to homogenize the different tenses and cases of each word. For example, \"walk\", \"walks\" and \"walking\" all become \"walk\".\n",
    "1. TF-IDF values are calculated for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 1.770897626876831 seconds\n",
      "[['a' 'c' 'b' 'd']\n",
      " ['b' 'c' 'a' 'd']\n",
      " ['c' 'b' 'a' 'd']\n",
      " ['d' 'c' 'b' 'a']]\n",
      "[[1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.27710268 0.27710268 0.        ]\n",
      " [1.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "basic_corpus = [\n",
    "    'Bees like to make honey',\n",
    "    'Bears like to eat honey',\n",
    "    \"Bees don't like bears\",\n",
    "    'Humans are walking around the park'\n",
    "]\n",
    "basic_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "tfidf_model = TfidfPredictor(lemmatize='custom')\n",
    "tfidf_model.fit(basic_corpus, basic_labels, verbose=True)\n",
    "pred, score = tfidf_model.predict(basic_corpus)\n",
    "print(pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data and Fitting TF-IDF Predictor\n",
    "\n",
    "We now download the training data and store it in a CSV, if it doesn't already exist. The training data is stored in [this S3 bucket](https://s3.console.aws.amazon.com/s3/buckets/amplifyobserverinsights-aoinsightslandingbucket29-5vcr471d4nm5). We extract the columns that are used for our corpus and labels, then feed it into a `TfidfPredictor` to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deserializing data from ./data/training_data.csv took 0.22211885452270508 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 79.89296960830688 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "df = get_data(r'./data/training_data.csv', force_redownload=False, verbose=True)\n",
    "\n",
    "corpus_col='title_body'\n",
    "url_col = 'url'\n",
    "title_col='title'\n",
    "\n",
    "# corpus = train_df[corpus_col]\n",
    "# labels = list(zip(train_df[url_col], train_df[title_col]))\n",
    "\n",
    "corpus, labels = get_corpus_labels(df, corpus_col, [url_col, title_col])\n",
    "\n",
    "tfidf_model = TfidfPredictor(lemmatize='custom')\n",
    "tfidf_model.fit(corpus, labels, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "Below, you can see the number of words in the vocabulary. This number is quite high, but the vectors being compared are \"sparse\" (i.e. having few non-zero values), so dot product computation is fast in the average case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocab words: 23685\n",
      "Available repos: ['docs', 'amplify-codegen', 'amplify-android', 'aws-appsync-realtime-client-ios', 'aws-amplify.github.io', 'amplify-ci-support', 'amplify-ui', 'amplify-js', 'aws-sdk-android', 'amplify-adminui', 'aws-sdk-ios', 'amplify-ios', 'amplify-console', 'amplify-cli', 'amplify-flutter', 'amplify-observer', 'amplify-js-samples', 'community']\n"
     ]
    }
   ],
   "source": [
    "# Model stats\n",
    "vocab = tfidf_model._vectorizer.get_feature_names()\n",
    "print('Number of words in vocabulary:', len(vocab))\n",
    "repo_list = list(set(df['repo']))\n",
    "print('Available repos:', repo_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing inferences\n",
    "\n",
    "Below, we perform an example query prediction. `TfidfPredictor.predict` returns a list of labels corresponding to the `n_best` vectors that are closest to the query's vector, and a list of scores that each of those labels attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction took 2.3255679607391357 seconds\n",
      "[[['https://github.com/aws-amplify/amplify-js/issues/8472'\n",
      "   'AmplifySignIn component does not work with password managers or native browser autofill']\n",
      "  ['https://github.com/aws-amplify/amplify-adminui/issues/233'\n",
      "   'Password managers, remember password, and suggest password not working in login form']\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/5782'\n",
      "   \"(React) UI Components don't support password managers\"]\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/8289'\n",
      "   \"Password managers don't seem to auto-fill login credentials in AmplifyAuthenticator -> AmplifySignIn\"]\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/4748'\n",
      "   \"[VueJS] Firefox autofill don't work\"]\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/3799'\n",
      "   'Password reset issue - chrome autofill']\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/14'\n",
      "   'Forgot password / change password?']\n",
      "  ['https://github.com/aws-amplify/aws-sdk-ios/issues/3076'\n",
      "   'Add Support to Swift Package Manager']\n",
      "  ['https://github.com/aws-amplify/aws-sdk-ios/issues/313'\n",
      "   'Support Swift Package Manager']\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/6111'\n",
      "   'Svelte UI Components']]]\n",
      "[[0.74256925 0.3380571  0.31577352 0.29732471 0.29658906 0.292673\n",
      "  0.28852257 0.28349613 0.26677107 0.26533534]]\n"
     ]
    }
   ],
   "source": [
    "query = 'AmplifySignIn component does not work with password managers or native browser autofill'\n",
    "pred, score = tfidf_model.predict(query, verbose=True)\n",
    "print(pred)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
