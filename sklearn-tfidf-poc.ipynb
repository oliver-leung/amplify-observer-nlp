{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TF-IDF Predictor with Scikit-learn: Proof of Concept\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency** (TF-IDF) is an information retrieval (IR) statistic used to determine how important a word is to a document. Simply put, we determine the TF-IDF of a word in a document by counting the occurrences of that word in the document, then dividing it by the number of documents in which it appears. Then, this process is repeated for each word in the document, resulting in a vector indicating the relevance of each word to that document. It's a simple, but effective IR method that was introduced in the 1980s, yet has stood the test of time.\n",
    "\n",
    "The bread and butter of IR is the process of turning arbitrary-length documents into fixed-length vectors (other such methods include word embeddings, encode-decode model, etc.). Once we have vectors that abstractly represent the semantics of documents, we can compare such vectors using metrics like cosine similarity, i.e. the dot product between two vectors. This principle then forms the basis of our project - *using TF-IDF, we map a small query and large documents into a high-dimensional vector space, then determine which documents are most relevant to the query*. In essence, it is a search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Update and import dependencies\n",
    "!pip install -Uqr requirements.txt\n",
    "\n",
    "# Data science/NLP packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Local modules\n",
    "import importlib\n",
    "import tfidf_predictor, utils_tfidf\n",
    "for m in [tfidf_predictor, utils_tfidf]:\n",
    "    importlib.reload(m)\n",
    "\n",
    "from tfidf_predictor import VectorSimilarity, TfidfPredictor\n",
    "from utils_tfidf import get_data, get_corpus_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending SKLearn: VectorSimilarity\n",
    "\n",
    "In order to perform pairwise comparisons across vectors and find the most similar pairs, we implemented the `VectorSimilarity` class, extending SKLearn's `Estimator` interface. The fitting process of `VectorSimilarity` takes two arrays:\n",
    "\n",
    "1. An array of float vectors, each with the same dimensionality\n",
    "1. An array of labels, which can have any type or shape.\n",
    "\n",
    "Note that both arrays must have equal length, and that the `i`th vector corresponds to the `i`th label for all `i`. Then, when we feed it an input vector, `VectorSimilarity` compares that vector against all of the vectors that it was fitted on, returning the corresponding labels of the `n_best` vectors. A \"score\" is also returned, which is just the dot product with each of the `n_best` vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) <class 'numpy.ndarray'>\n",
      "(4, 2)\n"
     ]
    },
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of argument at /root/amplify-observer-nlp/tfidf_predictor.py (48)\u001b[0m\n\u001b[1m\nFile \"tfidf_predictor.py\", line 48:\u001b[0m\n\u001b[1m    def linear_kernel_numba(self, u:np.ndarray, v:np.ndarray):\n\u001b[1m        assert(u.shape[0] == v.shape[0])\n\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n\nThis error may have been caused by the following argument(s):\n- argument 0: \u001b[1mCannot determine Numba type of <class 'tfidf_predictor.VectorSimilarity'>\u001b[0m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8bbce4782b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mvec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Shape needs to be (1, n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Most similar vectors:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confidence scores:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amplify-observer-nlp/tfidf_predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, verbose)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mgram_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_desc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gram_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram_desc_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_best\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgram_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_best\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/amplify-observer-nlp/tfidf_predictor.py\u001b[0m in \u001b[0;36m_gram_matrices\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mgram_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_kernel_numba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMemoryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mMemoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'too big {X.shape}, {self._Vectors.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0merror_rewrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'typing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;31m# Something unsupported is present in the user code, add help info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py\u001b[0m in \u001b[0;36merror_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    359\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0margtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mnon-precise type pyobject\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of argument at /root/amplify-observer-nlp/tfidf_predictor.py (48)\u001b[0m\n\u001b[1m\nFile \"tfidf_predictor.py\", line 48:\u001b[0m\n\u001b[1m    def linear_kernel_numba(self, u:np.ndarray, v:np.ndarray):\n\u001b[1m        assert(u.shape[0] == v.shape[0])\n\u001b[0m        \u001b[1m^\u001b[0m\u001b[0m\n\nThis error may have been caused by the following argument(s):\n- argument 0: \u001b[1mCannot determine Numba type of <class 'tfidf_predictor.VectorSimilarity'>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visual representation of vector locations\n",
    "\n",
    "       /\\\n",
    "        b  ?\n",
    "        |\n",
    "        |\n",
    "< c --------- a >\n",
    "        |\n",
    "        |\n",
    "        d\n",
    "       \\/\n",
    "\"\"\"\n",
    "X = np.array(\n",
    "    [[1, 0],\n",
    "     [0, 1],\n",
    "     [-1, 0],\n",
    "     [0, -1]]\n",
    ")\n",
    "y = np.array(['a', 'b', 'c', 'd'])\n",
    "\n",
    "sim_estimator = VectorSimilarity(n_best=10)\n",
    "sim_estimator = sim_estimator.fit(X, y)\n",
    "\n",
    "vec_input = np.array([0.5, 1]).reshape(1, -1) # Shape needs to be (1, n)\n",
    "pred, score = sim_estimator.predict(vec_input)\n",
    "print('Most similar vectors:\\n', pred)\n",
    "print('Confidence scores:\\n', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending SKLearn: TfidfPredictor\n",
    "\n",
    "To make similarity predictions on natural language, we use the above `VectorSimilarity` class in tandem with SKLearn's `TfidfVectorizer`, tying the two using SKLearn's `Pipeline` interface. We then wrapped the pipeline in another custom `Estimator`, `TfidfPredictor`, in order to have more control over the inputs and outputs of the model.\n",
    "\n",
    "The `TfidfVectorizer` takes in natural language documents and performs the following transformations:\n",
    "\n",
    "1. The document is separated into word tokens that are alphabetical and at least 3 characters long.\n",
    "1. Stop words such as articles, prepositions, and pronouns are removed.\n",
    "1. Lemmatization is applied to homogenize the different tenses and cases of each word. For example, \"walk\", \"walks\" and \"walking\" all become \"walk\".\n",
    "1. TF-IDF values are calculated for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "basic_corpus = [\n",
    "    'Bees like to make honey',\n",
    "    'Bears like to eat honey',\n",
    "    \"Bees don't like bears\",\n",
    "    'Humans are walking around the park'\n",
    "]\n",
    "basic_labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "tfidf_model = TfidfPredictor(lemmatize='custom')\n",
    "tfidf_model.fit(basic_corpus, basic_labels, verbose=True)\n",
    "pred, score = tfidf_model.predict(basic_corpus)\n",
    "print(pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data and Fitting TF-IDF Predictor\n",
    "\n",
    "We now download the training data and store it in a CSV, if it doesn't already exist. The training data is stored in [this S3 bucket](https://s3.console.aws.amazon.com/s3/buckets/amplifyobserverinsights-aoinsightslandingbucket29-5vcr471d4nm5). We extract the columns that are used for our corpus and labels, then feed it into a `TfidfPredictor` to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "df = get_data(r'./data/training_data.csv', force_redownload=False, verbose=True)[:100]\n",
    "\n",
    "corpus_col='title_body'\n",
    "url_col = 'url'\n",
    "title_col='title'\n",
    "\n",
    "# corpus = train_df[corpus_col]\n",
    "# labels = list(zip(train_df[url_col], train_df[title_col]))\n",
    "\n",
    "corpus, labels = get_corpus_labels(df, corpus_col, [url_col, title_col])\n",
    "\n",
    "tfidf_model = TfidfPredictor(lemmatize='custom')\n",
    "tfidf_model.fit(corpus, labels, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Statistics\n",
    "Below, you can see the number of words in the vocabulary. This number is quite high, but the vectors being compared are \"sparse\" (i.e. having few non-zero values), so dot product computation is fast in the average case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model stats\n",
    "vocab = tfidf_model._vectorizer.get_feature_names()\n",
    "print('Number of words in vocabulary:', len(vocab))\n",
    "repo_list = list(set(df['repo']))\n",
    "print('Available repos:', repo_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing inferences\n",
    "\n",
    "Below, we perform an example query prediction. `TfidfPredictor.predict` returns a list of labels corresponding to the `n_best` vectors that are closest to the query's vector, and a list of scores that each of those labels attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'AmplifySignIn component does not work with password managers or native browser autofill'\n",
    "pred, score = tfidf_model.predict(query, verbose=True)\n",
    "print(pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction took 2.426900863647461 seconds\n",
      "[[['https://github.com/aws-amplify/amplify-cli/issues/3114'\n",
      "   'DataStore generates subscriptions for all @model types']\n",
      "  ['https://github.com/aws-amplify/amplify-cli/issues/1070'\n",
      "   \"AppSync Subscriptions not working based on Parent's ID\"]\n",
      "  ['https://github.com/aws-amplify/amplify-android/issues/1388'\n",
      "   'Datastore model subscription fails due to timeout under slow network connection']\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/5050'\n",
      "   'Limit of 50 subscriptions reached Datastore']\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/4067'\n",
      "   'POSSIBLE BUG: 401 error with Amplify GraphQL Subscription']\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/5648'\n",
      "   'DataStore keeps searching for non-existent Subscriptions']\n",
      "  ['https://github.com/aws-amplify/amplify-js/issues/4683'\n",
      "   'Authed subscriptions not working']\n",
      "  ['https://github.com/aws-amplify/amplify-cli/issues/1810'\n",
      "   'GraphQL AppSync Subscriptions not generated by AWS Amplify for custom mutations']\n",
      "  ['https://github.com/aws-amplify/amplify-cli/issues/7033'\n",
      "   \"Public AWS_IAM Subscription Will Result 'Unauthorized' with @model(subscriptions: { level: public }) set\"]\n",
      "  ['https://github.com/aws-amplify/amplify-ios/issues/661'\n",
      "   'Listen in to a custom subscription within an AppSync schema']]]\n",
      "[[0.61311352 0.59705685 0.59430983 0.58164397 0.5797863  0.54849211\n",
      "  0.54453054 0.53468395 0.50478223 0.48397521]]\n"
     ]
    }
   ],
   "source": [
    "query = 'DataStore model subscription fails'\n",
    "pred, score = tfidf_model.predict(query, verbose=True)\n",
    "print(pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: add section showing inspection functions"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
